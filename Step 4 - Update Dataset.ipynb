{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING MODEL -  IROHACK PROJECT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Update the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1.1: Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📌 STEP 1: Update the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for 500 tickers...\n",
      "YF.download() has changed argument auto_adjust default to True\n",
      "✅ Data updated and merged successfully! File saved at: D:\\Project\\TradeVision\\Merged_Final_Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 📂 File paths\n",
    "final_dataset_path = r\"D:\\Project\\TradeVision\\Data\\Final Dataset\\Processing.csv\"\n",
    "\n",
    "# 📥 Load the main dataset\n",
    "TradeVision = pl.read_csv(final_dataset_path, try_parse_dates=True)\n",
    "\n",
    "# 📆 Define the update period (e.g., last 10 days)\n",
    "start_date = (datetime.today() - timedelta(days=10)).strftime('%Y-%m-%d')\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# 📌 Get all unique tickers from the dataset (S&P 500)\n",
    "tickers = TradeVision[\"Ticker\"].unique().to_list()\n",
    "\n",
    "# 📌 Add Ibovespa tickers\n",
    "ibov_tickers = [\n",
    "    \"PETR4.SA\", \"VALE3.SA\", \"ITUB4.SA\", \"BBDC4.SA\", \"ABEV3.SA\",\n",
    "    \"BBAS3.SA\", \"B3SA3.SA\", \"BRFS3.SA\", \"CSNA3.SA\", \"ELET3.SA\"\n",
    "]  # Add more tickers as needed\n",
    "\n",
    "# 🔄 Merge both lists (avoiding duplicates)\n",
    "tickers = list(set(tickers + ibov_tickers))\n",
    "\n",
    "# 📊 Create a list to store new data\n",
    "new_data = []\n",
    "\n",
    "# 🔄 Download stock data in batches to avoid rate limits\n",
    "try:\n",
    "    print(f\"Fetching data for {len(tickers)} tickers...\")\n",
    "\n",
    "    # Download data for all tickers at once\n",
    "    data = yf.download(tickers, start=start_date, end=end_date, progress=False, group_by=\"ticker\")\n",
    "\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            stock = yf.Ticker(ticker)\n",
    "            company_name = stock.info.get(\"longName\", \"Unknown\")  # Get company name\n",
    "\n",
    "            # Extract stock data\n",
    "            df = data[ticker].reset_index()\n",
    "            df[\"Ticker\"] = ticker\n",
    "            df[\"Company\"] = company_name\n",
    "\n",
    "            # Select relevant columns\n",
    "            df = df[[\"Date\", \"Ticker\", \"Company\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "            df.columns = [\"Date\", \"Ticker\", \"Company\", \"Open_Price\", \"High_Price\", \"Low_Price\", \"Close_Price\", \"Volume\"]\n",
    "\n",
    "            # 🛠️ Fix missing dates issue: Ensure we have all dates\n",
    "            df[\"Date\"] = pd.to_datetime(df[\"Date\"])  # Convert to datetime\n",
    "            df = df.set_index(\"Date\").asfreq(\"B\")  # Fill missing business days\n",
    "            df = df.reset_index()\n",
    "\n",
    "            new_data.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {ticker}: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error fetching stock data: {e}\")\n",
    "\n",
    "# 📊 Convert the list into a DataFrame\n",
    "if new_data:\n",
    "    Update = pl.from_pandas(pd.concat(new_data, ignore_index=True))\n",
    "\n",
    "    # Ensure the Date column is correctly formatted\n",
    "    Update = Update.with_columns(pl.col(\"Date\").cast(pl.Date))\n",
    "\n",
    "    # 🔗 Ensure Update has all columns from TradeVision\n",
    "    missing_cols = set(TradeVision.columns) - set(Update.columns)\n",
    "    for col in missing_cols:\n",
    "        Update = Update.with_columns(pl.lit(None).alias(col))\n",
    "\n",
    "    # Reorder columns to match TradeVision\n",
    "    Update = Update.select(TradeVision.columns)\n",
    "\n",
    "    # 🔗 Merge new data with TradeVision, keeping the correct order\n",
    "    TradeVision_updated = pl.concat([TradeVision, Update]).sort([\"Date\", \"Ticker\"])\n",
    "\n",
    "    # 💾 Save the updated dataset\n",
    "    output_path = r\"D:\\Project\\TradeVision\\Merged_Final_Dataset.csv\"\n",
    "    TradeVision_updated.write_csv(output_path)\n",
    "\n",
    "    print(f\"✅ Data updated and merged successfully! File saved at: {output_path}\")\n",
    "else:\n",
    "    print(\"⚠️ No new data was downloaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📌 STEP 1.1: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Step 1: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "✅ Dataset Successfully Loaded & Cleaned!\n",
      "==================================================\n",
      "📊 Total Rows: 36,092\n",
      "📊 Total Columns: 18\n",
      "==================================================\n",
      "\n",
      "🔹 **Preview of Cleaned Dataset:**\n",
      "\n",
      "╒════════════╤══════════╤═════════╤════════╤═══════╤════════╤══════════╤═══════════════════╤═══════════════════════════╤═════════════╤═════════════╤══════════════╤═════════╤════════════════════╤════════╤═══════════════════╤════════════════════╤═══════════════╕\n",
      "│ Date       │ Ticker   │   Close │   High │   Low │   Open │   Volume │   Daily_Variation │   Cumulative_Variation_10 │   MA_10days │   MA_50days │   MA_200days │ Month   │   Pct_Change_Month │   Year │   Pct_Change_Year │   Pct_Change_Daily │ Trend_Label   │\n",
      "╞════════════╪══════════╪═════════╪════════╪═══════╪════════╪══════════╪═══════════════════╪═══════════════════════════╪═════════════╪═════════════╪══════════════╪═════════╪════════════════════╪════════╪═══════════════════╪════════════════════╪═══════════════╡\n",
      "│ 2020-01-04 │ AZUL4.SA │   25.13 │  25.5  │ 24.71 │  25.28 │    25.28 │         nan       │                 nan       │     25.13   │     25.13   │      25.13   │ 2020-01 │          nan       │   2020 │         nan       │          nan       │ Stable        │\n",
      "├────────────┼──────────┼─────────┼────────┼───────┼────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-01-06 │ AZUL4.SA │   27.1  │  27.32 │ 25.23 │  25.4  │    25.4  │           7.83924 │                   7.83924 │     26.115  │     26.115  │      26.115  │ 2020-01 │            7.83924 │   2020 │           7.83924 │            7.83924 │ Stable        │\n",
      "├────────────┼──────────┼─────────┼────────┼───────┼────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-01-07 │ AZUL4.SA │   37.74 │  38.69 │ 37.72 │  38.65 │    38.65 │          39.262   │                  47.1012  │     29.99   │     29.99   │      29.99   │ 2020-01 │           39.262   │   2020 │          39.262   │           39.262   │ Stable        │\n",
      "├────────────┼──────────┼─────────┼────────┼───────┼────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-01-09 │ AZUL4.SA │   26.07 │  26.1  │ 25.25 │  25.83 │    25.83 │         -30.9221  │                  16.1791  │     29.01   │     29.01   │      29.01   │ 2020-01 │          -30.9221  │   2020 │         -30.9221  │          -30.9221  │ Stable        │\n",
      "├────────────┼──────────┼─────────┼────────┼───────┼────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-01-10 │ AZUL4.SA │   38.41 │  40.42 │ 38.41 │  40.26 │    40.26 │          47.3341  │                  63.5132  │     30.89   │     30.89   │      30.89   │ 2020-01 │           47.3341  │   2020 │          47.3341  │           47.3341  │ Stable        │\n",
      "├────────────┼──────────┼─────────┼────────┼───────┼────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-01-12 │ AZUL4.SA │   13.4  │  13.66 │ 13    │  13.3  │    13.3  │         -65.1133  │                  -1.60002 │     27.975  │     27.975  │      27.975  │ 2020-01 │          -65.1133  │   2020 │         -65.1133  │          -65.1133  │ Stable        │\n",
      "├────────────┼──────────┼─────────┼────────┼───────┼────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-02-01 │ AZUL4.SA │   14.85 │  16.09 │ 14.37 │  16.04 │    16.04 │          10.8209  │                   9.22089 │     26.1    │     26.1    │      26.1    │ 2020-02 │          nan       │   2020 │          10.8209  │           10.8209  │ Stable        │\n",
      "├────────────┼──────────┼─────────┼────────┼───────┼────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-02-03 │ AZUL4.SA │   59.07 │  61.95 │ 58.89 │  61.95 │    61.95 │         297.778   │                 306.999   │     30.2213 │     30.2213 │      30.2213 │ 2020-02 │          297.778   │   2020 │         297.778   │          297.778   │ Stable        │\n",
      "├────────────┼──────────┼─────────┼────────┼───────┼────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-02-04 │ AZUL4.SA │   41.39 │  43.29 │ 41.17 │  42.4  │    42.4  │         -29.9306  │                 277.068   │     31.4622 │     31.4622 │      31.4622 │ 2020-02 │          -29.9306  │   2020 │         -29.9306  │          -29.9306  │ Stable        │\n",
      "├────────────┼──────────┼─────────┼────────┼───────┼────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-02-06 │ AZUL4.SA │   21.86 │  24.5  │ 21.83 │  23.65 │    23.65 │         -47.1853  │                 229.883   │     30.502  │     30.502  │      30.502  │ 2020-02 │          -47.1853  │   2020 │         -47.1853  │          -47.1853  │ Stable        │\n",
      "╘════════════╧══════════╧═════════╧════════╧═══════╧════════╧══════════╧═══════════════════╧═══════════════════════════╧═════════════╧═════════════╧══════════════╧═════════╧════════════════════╧════════╧═══════════════════╧════════════════════╧═══════════════╛\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load dataset and remove unnamed columns\n",
    "file_path = r\"D:\\Project\\TradeVision\\Close_data.csv\"\n",
    "\n",
    "Final_Dataset = pd.read_csv(file_path, encoding='latin1', index_col=False, low_memory=False)\n",
    "\n",
    "# Drop any columns that have \"Unnamed\" in their name\n",
    "TradeVision = TradeVision.loc[:, ~TradeVision.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# Get dataset shape\n",
    "num_rows, num_cols = TradeVision.shape\n",
    "\n",
    "# Print dataset summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"✅ Dataset Successfully Loaded & Cleaned!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"📊 Total Rows: {num_rows:,}\")\n",
    "print(f\"📊 Total Columns: {num_cols}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Print the first 10 rows in a properly formatted table\n",
    "print(\"\\n🔹 **Preview of Cleaned Dataset:**\\n\")\n",
    "print(tabulate(TradeVision.head(10), headers=\"keys\", tablefmt=\"fancy_grid\", showindex=False))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Step 2: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 **Missing Data in Key Columns:**\n",
      "╒════╤════════════╤══════════════════╕\n",
      "│    │ Column     │   Missing Values │\n",
      "╞════╪════════════╪══════════════════╡\n",
      "│  0 │ Ticker     │                0 │\n",
      "├────┼────────────┼──────────────────┤\n",
      "│  1 │ Company    │                0 │\n",
      "├────┼────────────┼──────────────────┤\n",
      "│  2 │ Open_Price │                0 │\n",
      "├────┼────────────┼──────────────────┤\n",
      "│  3 │ Volume     │                0 │\n",
      "╘════╧════════════╧══════════════════╛\n",
      "\n",
      "🔍 **Duplicated Rows After Cleaning:** 0\n",
      "\n",
      "📊 **Cleaned Dataset Preview:**\n",
      "\n",
      "╒═════════════════════╤══════════╤════════════════════════════╤═════════════╤═══════════════╤══════════════╤══════════════╤═════════════╤═══════════════╤══════════╤═══════════════════╤═══════════════════════════╤═════════════╤═════════════╤══════════════╤═════════╤════════════════════╤════════╤═══════════════════╤════════════════════╤═══════════════╕\n",
      "│ Date                │ Ticker   │ Company                    │ Sector      │ Country       │   Open_Price │   High_Price │   Low_Price │   Close_Price │   Volume │   Daily_Variation │   Cumulative_Variation_10 │   MA_10days │   MA_50days │   MA_200days │ Month   │   Pct_Change_Month │   Year │   Pct_Change_Year │   Pct_Change_Daily │ Trend_Label   │\n",
      "╞═════════════════════╪══════════╪════════════════════════════╪═════════════╪═══════════════╪══════════════╪══════════════╪═════════════╪═══════════════╪══════════╪═══════════════════╪═══════════════════════════╪═════════════╪═════════════╪══════════════╪═════════╪════════════════════╪════════╪═══════════════════╪════════════════════╪═══════════════╡\n",
      "│ 2020-01-02 00:00:00 │ A        │ Agilent Technologies, Inc. │ Health Care │ United States │      83.013  │      83.4479 │     82.3366 │       83.0614 │  1410500 │        nan        │               nan         │     83.0614 │     83.0614 │      83.0614 │ 2020-01 │         nan        │   2020 │        nan        │         nan        │ Stable        │\n",
      "├─────────────────────┼──────────┼────────────────────────────┼─────────────┼───────────────┼──────────────┼──────────────┼─────────────┼───────────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-01-03 00:00:00 │ A        │ Agilent Technologies, Inc. │ Health Care │ United States │      81.8244 │      82.4622 │     81.6601 │       81.7277 │  1118300 │         -1.60561  │                -1.60561   │     82.3945 │     82.3945 │      82.3945 │ 2020-01 │          -1.60561  │   2020 │         -1.60561  │          -1.60561  │ Stable        │\n",
      "├─────────────────────┼──────────┼────────────────────────────┼─────────────┼───────────────┼──────────────┼──────────────┼─────────────┼───────────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-01-06 00:00:00 │ A        │ Agilent Technologies, Inc. │ Health Care │ United States │      81.1769 │      81.9693 │     80.7903 │       81.9693 │  1993200 │          0.295616 │                -1.30999   │     82.2528 │     82.2528 │      82.2528 │ 2020-01 │           0.295616 │   2020 │          0.295616 │           0.295616 │ Stable        │\n",
      "├─────────────────────┼──────────┼────────────────────────────┼─────────────┼───────────────┼──────────────┼──────────────┼─────────────┼───────────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-01-07 00:00:00 │ A        │ Agilent Technologies, Inc. │ Health Care │ United States │      81.1382 │      82.3945 │     81.1189 │       82.2206 │  1684700 │          0.306528 │                -1.00347   │     82.2447 │     82.2447 │      82.2447 │ 2020-01 │           0.306528 │   2020 │          0.306528 │           0.306528 │ Stable        │\n",
      "├─────────────────────┼──────────┼────────────────────────────┼─────────────┼───────────────┼──────────────┼──────────────┼─────────────┼───────────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-01-08 00:00:00 │ A        │ Agilent Technologies, Inc. │ Health Care │ United States │      83.071  │      83.5639 │     82.3366 │       83.0324 │  1847600 │          0.987333 │                -0.0161333 │     82.4023 │     82.4023 │      82.4023 │ 2020-01 │           0.987333 │   2020 │          0.987333 │           0.987333 │ Stable        │\n",
      "├─────────────────────┼──────────┼────────────────────────────┼─────────────┼───────────────┼──────────────┼──────────────┼─────────────┼───────────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-01-09 00:00:00 │ A        │ Agilent Technologies, Inc. │ Health Care │ United States │      83.5542 │      84.7525 │     83.2739 │       84.337  │  1912700 │          1.57121  │                 1.55507   │     82.7247 │     82.7247 │      82.7247 │ 2020-01 │           1.57121  │   2020 │          1.57121  │           1.57121  │ Stable        │\n",
      "├─────────────────────┼──────────┼────────────────────────────┼─────────────┼───────────────┼──────────────┼──────────────┼─────────────┼───────────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-01-10 00:00:00 │ A        │ Agilent Technologies, Inc. │ Health Care │ United States │      84.7719 │      85.2744 │     84.3853 │       84.6462 │  1417000 │          0.366684 │                 1.92176   │     82.9992 │     82.9992 │      82.9992 │ 2020-01 │           0.366684 │   2020 │          0.366684 │           0.366684 │ Stable        │\n",
      "├─────────────────────┼──────────┼────────────────────────────┼─────────────┼───────────────┼──────────────┼──────────────┼─────────────┼───────────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-02-03 00:00:00 │ A        │ Agilent Technologies, Inc. │ Health Care │ United States │      80.4907 │      80.8773 │     79.3504 │       79.3891 │  1919800 │         -6.21075  │                -4.28899   │     82.5479 │     82.5479 │      82.5479 │ 2020-02 │         nan        │   2020 │         -6.21075  │          -6.21075  │ Stable        │\n",
      "├─────────────────────┼──────────┼────────────────────────────┼─────────────┼───────────────┼──────────────┼──────────────┼─────────────┼───────────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-02-04 00:00:00 │ A        │ Agilent Technologies, Inc. │ Health Care │ United States │      80.0655 │      81.0803 │     79.9979 │       80.713  │  1676000 │          1.66768  │                -2.62131   │     82.3441 │     82.3441 │      82.3441 │ 2020-02 │           1.66768  │   2020 │          1.66768  │           1.66768  │ Stable        │\n",
      "├─────────────────────┼──────────┼────────────────────────────┼─────────────┼───────────────┼──────────────┼──────────────┼─────────────┼───────────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-02-05 00:00:00 │ A        │ Agilent Technologies, Inc. │ Health Care │ United States │      81.689  │      82.6265 │     81.5538 │       82.0756 │  2345100 │          1.68819  │                -0.933118  │     82.3172 │     82.3172 │      82.3172 │ 2020-02 │           1.68819  │   2020 │          1.68819  │           1.68819  │ Stable        │\n",
      "╘═════════════════════╧══════════╧════════════════════════════╧═════════════╧═══════════════╧══════════════╧══════════════╧═════════════╧═══════════════╧══════════╧═══════════════════╧═══════════════════════════╧═════════════╧═════════════╧══════════════╧═════════╧════════════════════╧════════╧═══════════════════╧════════════════════╧═══════════════╛\n",
      "\n",
      "✅ **Cleaned Dataset Saved Successfully!** 📂\n",
      "Path: D:\\Project\\TradeVision\\Data\\Final Dataset\\Final_Dataset_Cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "Final_Dataset = pd.read_csv(r\"D:\\Project\\TradeVision\\Data\\Final Dataset\\Final_Dataset.csv\")\n",
    "\n",
    "# Convert Date to datetime and remove invalid rows\n",
    "Final_Dataset[\"Date\"] = pd.to_datetime(Final_Dataset[\"Date\"], errors=\"coerce\", dayfirst=True)\n",
    "Final_Dataset.dropna(subset=[\"Date\"], inplace=True)\n",
    "\n",
    "# Numeric columns that shouldn't have NaNs\n",
    "num_cols = [\"Open_Price\", \"High_Price\", \"Low_Price\", \"Close_Price\", \"Volume\"]\n",
    "\n",
    "# Step 1: Replace invalid values (≤ 0) with NaN\n",
    "Final_Dataset[num_cols] = Final_Dataset[num_cols].where(Final_Dataset[num_cols] > 0, np.nan)\n",
    "\n",
    "# Step 2: Fill missing values with column mean\n",
    "Final_Dataset[num_cols] = Final_Dataset[num_cols].fillna(Final_Dataset[num_cols].mean())\n",
    "\n",
    "# Convert Volume to integer (after filling NaNs)\n",
    "Final_Dataset[\"Volume\"] = Final_Dataset[\"Volume\"].astype(int)\n",
    "\n",
    "# Remove duplicates based on Ticker & Date\n",
    "Final_Dataset.drop_duplicates(subset=[\"Ticker\", \"Date\"], inplace=True)\n",
    "\n",
    "# Final Check for Missing Data\n",
    "print(\"\\n🔍 **Missing Data in Key Columns:**\")\n",
    "missing_data = Final_Dataset[[\"Ticker\", \"Company\", \"Open_Price\", \"Volume\"]].isnull().sum()\n",
    "print(tabulate(missing_data.reset_index(), headers=[\"Column\", \"Missing Values\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "# Final Check for Duplicates\n",
    "duplicates = Final_Dataset.duplicated().sum()\n",
    "print(f\"\\n🔍 **Duplicated Rows After Cleaning:** {duplicates:,}\")\n",
    "\n",
    "# ✅ Display Cleaned Dataset Preview\n",
    "print(\"\\n📊 **Cleaned Dataset Preview:**\\n\")\n",
    "print(tabulate(Final_Dataset.head(10), headers=\"keys\", tablefmt=\"fancy_grid\", showindex=False))\n",
    "\n",
    "# Save the cleaned dataset\n",
    "cleaned_file_path = r\"D:\\Project\\TradeVision\\Data\\Final Dataset\\Final_Dataset_Cleaned.csv\"\n",
    "Final_Dataset.to_csv(cleaned_file_path, index=False)\n",
    "print(f\"\\n✅ **Cleaned Dataset Saved Successfully!** 📂\\nPath: {cleaned_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📌 STEP 1.2: Add Necessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ **Column Names in Dataset:**\n",
      "Index(['Date', 'Ticker', 'Close', 'High', 'Low', 'Open', 'Volume'], dtype='object')\n",
      "\n",
      "============================================================\n",
      "✅ Dataset Updated & Saved! 📊\n",
      "🔹 Total Rows: 36,092 | 🔹 Total Columns: 18\n",
      "============================================================\n",
      "\n",
      "📊 **Updated Dataset Preview:**\n",
      "\n",
      "╒═════════════════════╤══════════╤═════════╤════════╤═══════╤════════╤══════════╤═══════════════════╤═══════════════════════════╤═════════════╤═════════════╤══════════════╤═════════╤════════════════════╤════════╤═══════════════════╤════════════════════╤═══════════════╕\n",
      "│ Date                │ Ticker   │   Close │   High │   Low │   Open │   Volume │   Daily_Variation │   Cumulative_Variation_10 │   MA_10days │   MA_50days │   MA_200days │ Month   │   Pct_Change_Month │ Year   │   Pct_Change_Year │   Pct_Change_Daily │ Trend_Label   │\n",
      "╞═════════════════════╪══════════╪═════════╪════════╪═══════╪════════╪══════════╪═══════════════════╪═══════════════════════════╪═════════════╪═════════════╪══════════════╪═════════╪════════════════════╪════════╪═══════════════════╪════════════════════╪═══════════════╡\n",
      "│ 2020-01-04 00:00:00 │ AZUL4.SA │   25.13 │  25.5  │ 24.71 │  25.28 │    25.28 │         nan       │                 nan       │     25.13   │     25.13   │      25.13   │ 2020-01 │          nan       │ 2020   │         nan       │          nan       │ Stable        │\n",
      "├─────────────────────┼──────────┼─────────┼────────┼───────┼────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-01-06 00:00:00 │ AZUL4.SA │   27.1  │  27.32 │ 25.23 │  25.4  │    25.4  │           7.83924 │                   7.83924 │     26.115  │     26.115  │      26.115  │ 2020-01 │            7.83924 │ 2020   │           7.83924 │            7.83924 │ Stable        │\n",
      "├─────────────────────┼──────────┼─────────┼────────┼───────┼────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-01-07 00:00:00 │ AZUL4.SA │   37.74 │  38.69 │ 37.72 │  38.65 │    38.65 │          39.262   │                  47.1012  │     29.99   │     29.99   │      29.99   │ 2020-01 │           39.262   │ 2020   │          39.262   │           39.262   │ Stable        │\n",
      "├─────────────────────┼──────────┼─────────┼────────┼───────┼────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-01-09 00:00:00 │ AZUL4.SA │   26.07 │  26.1  │ 25.25 │  25.83 │    25.83 │         -30.9221  │                  16.1791  │     29.01   │     29.01   │      29.01   │ 2020-01 │          -30.9221  │ 2020   │         -30.9221  │          -30.9221  │ Stable        │\n",
      "├─────────────────────┼──────────┼─────────┼────────┼───────┼────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-01-10 00:00:00 │ AZUL4.SA │   38.41 │  40.42 │ 38.41 │  40.26 │    40.26 │          47.3341  │                  63.5132  │     30.89   │     30.89   │      30.89   │ 2020-01 │           47.3341  │ 2020   │          47.3341  │           47.3341  │ Stable        │\n",
      "├─────────────────────┼──────────┼─────────┼────────┼───────┼────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-01-12 00:00:00 │ AZUL4.SA │   13.4  │  13.66 │ 13    │  13.3  │    13.3  │         -65.1133  │                  -1.60002 │     27.975  │     27.975  │      27.975  │ 2020-01 │          -65.1133  │ 2020   │         -65.1133  │          -65.1133  │ Stable        │\n",
      "├─────────────────────┼──────────┼─────────┼────────┼───────┼────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-02-01 00:00:00 │ AZUL4.SA │   14.85 │  16.09 │ 14.37 │  16.04 │    16.04 │          10.8209  │                   9.22089 │     26.1    │     26.1    │      26.1    │ 2020-02 │          nan       │ 2020   │          10.8209  │           10.8209  │ Stable        │\n",
      "├─────────────────────┼──────────┼─────────┼────────┼───────┼────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-02-03 00:00:00 │ AZUL4.SA │   59.07 │  61.95 │ 58.89 │  61.95 │    61.95 │         297.778   │                 306.999   │     30.2213 │     30.2213 │      30.2213 │ 2020-02 │          297.778   │ 2020   │         297.778   │          297.778   │ Stable        │\n",
      "├─────────────────────┼──────────┼─────────┼────────┼───────┼────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-02-04 00:00:00 │ AZUL4.SA │   41.39 │  43.29 │ 41.17 │  42.4  │    42.4  │         -29.9306  │                 277.068   │     31.4622 │     31.4622 │      31.4622 │ 2020-02 │          -29.9306  │ 2020   │         -29.9306  │          -29.9306  │ Stable        │\n",
      "├─────────────────────┼──────────┼─────────┼────────┼───────┼────────┼──────────┼───────────────────┼───────────────────────────┼─────────────┼─────────────┼──────────────┼─────────┼────────────────────┼────────┼───────────────────┼────────────────────┼───────────────┤\n",
      "│ 2020-02-06 00:00:00 │ AZUL4.SA │   21.86 │  24.5  │ 21.83 │  23.65 │    23.65 │         -47.1853  │                 229.883   │     30.502  │     30.502  │      30.502  │ 2020-02 │          -47.1853  │ 2020   │         -47.1853  │          -47.1853  │ Stable        │\n",
      "╘═════════════════════╧══════════╧═════════╧════════╧═══════╧════════╧══════════╧═══════════════════╧═══════════════════════════╧═════════════╧═════════════╧══════════════╧═════════╧════════════════════╧════════╧═══════════════════╧════════════════════╧═══════════════╛\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load the dataset with encoding fix\n",
    "file_path = r\"D:\\Project\\TradeVision\\Close_data.csv\"\n",
    "Final_Dataset = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "\n",
    "# Print column names to check\n",
    "print(\"\\n✅ **Column Names in Dataset:**\")\n",
    "print(Final_Dataset.columns)\n",
    "\n",
    "# Ensure 'Date' is in datetime format\n",
    "Final_Dataset['Date'] = pd.to_datetime(Final_Dataset['Date'], errors='coerce')\n",
    "\n",
    "# Check for NaN values in important columns\n",
    "if Final_Dataset['Close'].isna().sum() > 0:\n",
    "    print(\"⚠️ Warning: 'Close' column has missing values! Filling NaNs...\")\n",
    "    Final_Dataset['Close'] = Final_Dataset['Close'].fillna(method='ffill')\n",
    "\n",
    "if Final_Dataset['Ticker'].isna().sum() > 0:\n",
    "    print(\"⚠️ Warning: 'Ticker' column has missing values! Dropping rows...\")\n",
    "    Final_Dataset = Final_Dataset.dropna(subset=['Ticker'])\n",
    "\n",
    "# Sort data\n",
    "Final_Dataset = Final_Dataset.sort_values(by=['Ticker', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# 1️⃣ **Daily Variation (%)**\n",
    "Final_Dataset['Daily_Variation'] = Final_Dataset.groupby('Ticker')['Close'].pct_change() * 100\n",
    "\n",
    "# 2️⃣ **Cumulative Variation (10-day rolling sum)**\n",
    "Final_Dataset['Cumulative_Variation_10'] = Final_Dataset.groupby('Ticker')['Daily_Variation'].rolling(window=10, min_periods=1).sum().reset_index(level=0, drop=True)\n",
    "\n",
    "# 3️⃣ **Moving Averages**\n",
    "Final_Dataset['MA_10days'] = Final_Dataset.groupby('Ticker')['Close'].rolling(window=10, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "Final_Dataset['MA_50days'] = Final_Dataset.groupby('Ticker')['Close'].rolling(window=50, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "Final_Dataset['MA_200days'] = Final_Dataset.groupby('Ticker')['Close'].rolling(window=200, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# 4️⃣ **Monthly & Yearly Percentage Change**\n",
    "Final_Dataset['Month'] = Final_Dataset['Date'].dt.to_period('M')\n",
    "Final_Dataset['Pct_Change_Month'] = Final_Dataset.groupby(['Ticker', 'Month'])['Close'].pct_change() * 100\n",
    "\n",
    "Final_Dataset['Year'] = Final_Dataset['Date'].dt.to_period('Y')\n",
    "Final_Dataset['Pct_Change_Year'] = Final_Dataset.groupby(['Ticker', 'Year'])['Close'].pct_change() * 100\n",
    "\n",
    "# 5️⃣ **Daily Percentage Change**\n",
    "Final_Dataset['Pct_Change_Daily'] = Final_Dataset['Daily_Variation']\n",
    "\n",
    "# 6️⃣ **Trend Label**\n",
    "threshold = 0.5\n",
    "Final_Dataset['Trend_Label'] = 'Stable'\n",
    "Final_Dataset.loc[Final_Dataset['MA_10days'] > Final_Dataset['MA_50days'] + threshold, 'Trend_Label'] = 'High'\n",
    "Final_Dataset.loc[Final_Dataset['MA_10days'] < Final_Dataset['MA_50days'] - threshold, 'Trend_Label'] = 'Low'\n",
    "\n",
    "# Remove 'Unnamed' columns\n",
    "Final_Dataset = Final_Dataset.loc[:, ~Final_Dataset.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# Save cleaned dataset\n",
    "output_file_path = r\"D:\\Project\\TradeVision\\Close_data.csv\"\n",
    "Final_Dataset.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Display summary\n",
    "num_rows, num_cols = Final_Dataset.shape\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"✅ Dataset Updated & Saved! 📊\\n🔹 Total Rows: {num_rows:,} | 🔹 Total Columns: {num_cols}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Preview dataset\n",
    "print(\"\\n📊 **Updated Dataset Preview:**\\n\")\n",
    "print(tabulate(Final_Dataset.head(10), headers=\"keys\", tablefmt=\"fancy_grid\", showindex=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: Missing Values in MACD columns:\n",
      "MACD_Line         0\n",
      "MACD_Signal       0\n",
      "MACD_Histogram    0\n",
      "dtype: int64\n",
      "Debug: Missing Values in Bollinger Bands columns:\n",
      "Bollinger_MA         0\n",
      "Bollinger_Upper    500\n",
      "Bollinger_Lower    500\n",
      "dtype: int64\n",
      "✅ Feature engineering completed! New dataset saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "Final_Dataset = pd.read_csv(r\"D:\\Project\\TradeVision\\Data\\Final Dataset\\Final_Dataset.csv\")\n",
    "\n",
    "# Ensure the dataset is sorted by Date and Ticker\n",
    "Final_Dataset['Date'] = pd.to_datetime(Final_Dataset['Date'], errors='coerce')\n",
    "Final_Dataset.sort_values(by=['Date', 'Ticker'], inplace=True)\n",
    "\n",
    "### 📌 1️⃣ Lag Features\n",
    "Final_Dataset['Close_Price_t-1'] = Final_Dataset.groupby('Ticker')['Close_Price'].shift(1)\n",
    "Final_Dataset['Close_Price_t-5'] = Final_Dataset.groupby('Ticker')['Close_Price'].shift(5)\n",
    "Final_Dataset['Close_Price_t-10'] = Final_Dataset.groupby('Ticker')['Close_Price'].shift(10)\n",
    "\n",
    "### 📌 2️⃣ Relative Strength Index (RSI - 14 days)\n",
    "def compute_rsi(series, period=14):\n",
    "    delta = series.diff()\n",
    "    gain = np.where(delta > 0, delta, 0)\n",
    "    loss = np.where(delta < 0, -delta, 0)\n",
    "\n",
    "    avg_gain = pd.Series(gain, index=series.index).rolling(window=period, min_periods=1).mean()\n",
    "    avg_loss = pd.Series(loss, index=series.index).rolling(window=period, min_periods=1).mean()\n",
    "\n",
    "    rs = avg_gain / (avg_loss + 1e-10)  # Avoid division by zero\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "Final_Dataset['RSI_14'] = Final_Dataset.groupby('Ticker')['Close_Price'].transform(lambda x: compute_rsi(x))\n",
    "\n",
    "### 📌 3️⃣ Moving Average Convergence Divergence (MACD)\n",
    "def compute_macd(series, short_window=12, long_window=26, signal_window=9):\n",
    "    short_ema = series.ewm(span=short_window, adjust=False).mean()\n",
    "    long_ema = series.ewm(span=long_window, adjust=False).mean()\n",
    "    macd_line = short_ema - long_ema\n",
    "    signal_line = macd_line.ewm(span=signal_window, adjust=False).mean()\n",
    "    return macd_line, signal_line\n",
    "\n",
    "# Apply MACD using transform instead of apply\n",
    "Final_Dataset['MACD_Line'] = Final_Dataset.groupby('Ticker')['Close_Price'].transform(lambda x: compute_macd(x)[0])\n",
    "Final_Dataset['MACD_Signal'] = Final_Dataset.groupby('Ticker')['Close_Price'].transform(lambda x: compute_macd(x)[1])\n",
    "Final_Dataset['MACD_Histogram'] = Final_Dataset['MACD_Line'] - Final_Dataset['MACD_Signal']\n",
    "\n",
    "# Debug: Check for missing values in MACD columns\n",
    "print(\"Debug: Missing Values in MACD columns:\")\n",
    "print(Final_Dataset[['MACD_Line', 'MACD_Signal', 'MACD_Histogram']].isnull().sum())\n",
    "\n",
    "### 📌 4️⃣ Bollinger Bands (20-day period)\n",
    "def compute_bollinger_bands(series, window=20, num_std=2):\n",
    "    sma = series.rolling(window=window, min_periods=1).mean()\n",
    "    std = series.rolling(window=window, min_periods=1).std()\n",
    "    upper_band = sma + (num_std * std)\n",
    "    lower_band = sma - (num_std * std)\n",
    "    return sma, upper_band, lower_band\n",
    "\n",
    "# Apply Bollinger Bands using transform\n",
    "Final_Dataset['Bollinger_MA'] = Final_Dataset.groupby('Ticker')['Close_Price'].transform(lambda x: compute_bollinger_bands(x)[0])\n",
    "Final_Dataset['Bollinger_Upper'] = Final_Dataset.groupby('Ticker')['Close_Price'].transform(lambda x: compute_bollinger_bands(x)[1])\n",
    "Final_Dataset['Bollinger_Lower'] = Final_Dataset.groupby('Ticker')['Close_Price'].transform(lambda x: compute_bollinger_bands(x)[2])\n",
    "\n",
    "# Debug: Check for missing values in Bollinger Bands columns\n",
    "print(\"Debug: Missing Values in Bollinger Bands columns:\")\n",
    "print(Final_Dataset[['Bollinger_MA', 'Bollinger_Upper', 'Bollinger_Lower']].isnull().sum())\n",
    "\n",
    "### 📌 Save the new dataset with additional features\n",
    "Final_Dataset.to_csv(r\"D:\\Project\\TradeVision\\Data\\Final Dataset\\Final_Dataset_Features.csv\", index=False)\n",
    "\n",
    "print(\"✅ Feature engineering completed! New dataset saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📌 STEP 1.4: Connect DataFrame to an SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Engine Created!\n",
      "✅ Data uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Define your database name and table\n",
    "database_name = \"StockData\"  # Replace with actual database name\n",
    "table_name = \"StockData\"  # Replace with actual table name\n",
    "\n",
    "# Corrected connection string\n",
    "conn_str = f\"mssql+pyodbc://@localhost\\\\SQLExpress08/{database_name}?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes\"\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "try:\n",
    "    engine = create_engine(conn_str)\n",
    "    print(\"✅ Engine Created!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating engine: {e}\")\n",
    "\n",
    "# Assuming your DataFrame is named 'TradeVision'\n",
    "try:\n",
    "    TradeVision.to_sql(table_name, con=engine, index=False, if_exists='replace')  # Use 'replace' or 'append'\n",
    "    print(\"✅ Data uploaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error uploading data: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### //////////////////////////////////////// If Need it //////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📌 STEP 1.2: Trend Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train sample: 117597    High\n",
      "164545    High\n",
      "53334     High\n",
      "53741      Low\n",
      "30326     High\n",
      "Name: Trend_Label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract 'Trend_Label' column\n",
    "y = Final_Dataset['Trend_Label']\n",
    "\n",
    "# Split the data into training and testing sets (you can adjust the test size as per your needs)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = Final_Dataset.drop(columns=['Trend_Label'])  # Features\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now we have y_train and y_test\n",
    "print(\"y_train sample:\", y_train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📌 STEP 1.3: Mapping Trend_Label to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded y_train values: [ 1  1  1 -1  1  1  1  1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trend_mapping = {'High': 1, 'Low': -1, 'Stable': 0}\n",
    "\n",
    "# Apply the mapping to y_train and y_test\n",
    "y_train_encoded = np.array([trend_mapping[label] for label in y_train])\n",
    "y_test_encoded = np.array([trend_mapping[label] for label in y_test])\n",
    "\n",
    "# Check the first few encoded values\n",
    "print(\"Encoded y_train values:\", y_train_encoded[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📌 STEP 1.4: Find Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Data in each column:\n",
      "Date                       4000\n",
      "Ticker                        0\n",
      "Company                       0\n",
      "Open_Price                    0\n",
      "High_Price                    0\n",
      "Low_Price                     0\n",
      "Close_Price                   0\n",
      "Volume                        0\n",
      "Daily_Variation               1\n",
      "Cumulative_Variation_10      10\n",
      "MA_10days                     9\n",
      "MA_50days                    49\n",
      "MA_200days                  199\n",
      "Month                      4000\n",
      "Pct_Change_Month           4065\n",
      "Year                       4000\n",
      "Pct_Change_Year            4006\n",
      "Pct_Change_Daily              1\n",
      "Trend_Label                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (adjust the path as needed)\n",
    "Final_Dataset = pd.read_csv(r\"D:\\Project\\TradeVision\\Data\\Final Dataset\\Final_Dataset.csv\")\n",
    "\n",
    "# Check for missing values in the dataset\n",
    "missing_data = Final_Dataset.isnull().sum()\n",
    "\n",
    "# Print missing data statistics\n",
    "print(\"Missing Data in each column:\")\n",
    "print(missing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final dataset saved successfully to: D:\\Project\\TradeVision\\Data\\Final Dataset\\Final_Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the Final_Dataset to a CSV file\n",
    "output_file_path = r\"D:\\Project\\TradeVision\\Data\\Final Dataset\\Final_Dataset.csv\"\n",
    "Final_Dataset.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"✅ Final dataset saved successfully to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1.2: Transform the Data to a long Format Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (skip the first two rows, and set proper header)\n",
    "df = pd.read_csv(r\"D:\\Project\\TradeVision\\Volume.csv\", header=2)  # The correct header is row 3 (index 2)\n",
    "\n",
    "# Set the first column as the 'Date' column\n",
    "df.rename(columns={df.columns[0]: 'Date'}, inplace=True)\n",
    "\n",
    "# Check the first few rows to ensure tickers and dates are properly loaded\n",
    "print(df.head())\n",
    "\n",
    "# Extract tickers from the first row (row with index 0, since header=1)\n",
    "tickers = df.columns[1:]  # Get tickers from the columns, starting from the second column\n",
    "dates = df['Date']  # The 'Date' column is still intact\n",
    "\n",
    "# Prepare a list to collect the long format data\n",
    "long_format_data = []\n",
    "\n",
    "# Loop through each ticker and assign it to the dates correctly\n",
    "for ticker in tickers:\n",
    "    for date in dates:\n",
    "        if date != 'Date':  # Skip any rows that contain the word \"Date\"\n",
    "            # Append the date, ticker, and close value to the list\n",
    "            Open_value = df.loc[df['Date'] == date, ticker].values[0]  # Get the close price for the given ticker and date\n",
    "            long_format_data.append([date, ticker, Open_value])\n",
    "\n",
    "\n",
    "# Create the DataFrame from the long format data\n",
    "long_format_df = pd.DataFrame(long_format_data, columns=['Date', 'Ticker', 'Close'])\n",
    "\n",
    "# Save the result to a CSV file (optional)\n",
    "long_format_df.to_csv('Close_data.csv', index=False)\n",
    "\n",
    "# Display the first few rows to check the results\n",
    "print(long_format_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poll for New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m     check_and_import()\n\u001b[1;32m---> 39\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait for 10 seconds before checking again\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from SQLConnection import SQLConnection  # Import the function with the correct name\n",
    "\n",
    "# Path to the dataset (Update.csv)\n",
    "file_path = r\"D:\\Project\\TradeVision\\Data\\Final Dataset\\Final_Dataset.csv\"  # Use raw string for the path\n",
    "\n",
    "# Name of the table where data will be inserted\n",
    "table_name = \"CreateTable\"  # Replace with your actual table name in the database\n",
    "\n",
    "def check_and_import():\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the new data from Update.csv\n",
    "        update_data = pd.read_csv(file_path)\n",
    "\n",
    "        # Create a database connection using the function from SQLConnection.py\n",
    "        conn = SQLConnection()  # Use the correct function name\n",
    "\n",
    "        if conn:\n",
    "            # Import data into the SQL table (using pyodbc)\n",
    "            try:\n",
    "                update_data.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "                print(f\"Data imported successfully into {table_name}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error while importing data: {e}\")\n",
    "            finally:\n",
    "                # Close the connection\n",
    "                conn.close()\n",
    "        else:\n",
    "            print(\"Connection to SQL Server failed.\")\n",
    "\n",
    "        # Optionally, remove the file after processing (if desired)\n",
    "        os.remove(file_path)\n",
    "\n",
    "# Polling loop: check every X seconds (e.g., 10 seconds)\n",
    "while True:\n",
    "    check_and_import()\n",
    "    time.sleep(10)  # Wait for 10 seconds before checking again\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date Ticker Company  Open_Price  High_Price   Low_Price  Close_Price  \\\n",
      "0  2020-02-01    MMM      3M  120.780382  122.364235  120.413308   122.357437   \n",
      "1  2020-03-01    MMM      3M  120.331736  121.446543  119.386858   121.303795   \n",
      "2  2020-06-01    MMM      3M  120.420099  121.480533  119.876285   121.419350   \n",
      "3  2020-07-01    MMM      3M  121.188228  121.344585  120.195775   120.929924   \n",
      "4  2020-08-01    MMM      3M  120.997903  123.377075  120.759980   122.785675   \n",
      "\n",
      "    Volume  Daily_Variation  Cumulative_Variation_10  MA_10days  MA_50days  \\\n",
      "0  4307633              NaN                      NaN        NaN        NaN   \n",
      "1  2950412        -0.861118                      NaN        NaN        NaN   \n",
      "2  2389608         0.095261                      NaN        NaN        NaN   \n",
      "3  2598908        -0.403087                      NaN        NaN        NaN   \n",
      "4  3298927         1.534567                      NaN        NaN        NaN   \n",
      "\n",
      "   MA_200days    Year    Month  Pct_Change_Daily  Pct_Change_Month  \\\n",
      "0         NaN  2020.0  2020-02               NaN               NaN   \n",
      "1         NaN  2020.0  2020-03         -0.861118               NaN   \n",
      "2         NaN  2020.0  2020-06          0.095261               NaN   \n",
      "3         NaN  2020.0  2020-07         -0.403087               NaN   \n",
      "4         NaN  2020.0  2020-08          1.534567               NaN   \n",
      "\n",
      "   Pct_Change_Year Trend_Label  \n",
      "0              NaN      Stable  \n",
      "1        -0.861118      Stable  \n",
      "2         0.095261      Stable  \n",
      "3        -0.403087      Stable  \n",
      "4         1.534567      Stable  \n"
     ]
    }
   ],
   "source": [
    "# Clean missing or invalid values (for example, replacing NaNs with None)\n",
    "update_data = update_data.fillna({'Column_Name': None})  # Replace with None for columns that can accept NULL\n",
    "\n",
    "# Ensure numeric columns are correct type\n",
    "update_data[\"Open_Price\"] = pd.to_numeric(update_data[\"Open_Price\"], errors='coerce')\n",
    "update_data[\"Close_Price\"] = pd.to_numeric(update_data[\"Close_Price\"], errors='coerce')\n",
    "update_data[\"Volume\"] = pd.to_numeric(update_data[\"Volume\"], errors='coerce')\n",
    "# You can apply this similarly to other numerical columns\n",
    "\n",
    "# Re-check the DataFrame after cleaning\n",
    "print(update_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cleid\\AppData\\Local\\Temp\\ipykernel_17052\\3903671677.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  update_data[\"Trend_Label\"].fillna(\"Unknown\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Fill NaN values in the 'Year' column with a placeholder (e.g., 0)\n",
    "update_data[\"Year\"] = update_data[\"Year\"].fillna(0).astype(int)\n",
    "\n",
    "# Convert 'Month' to an integer (extract the month)\n",
    "update_data[\"Month\"] = pd.to_datetime(update_data[\"Month\"], errors='coerce').dt.month\n",
    "\n",
    "# Fill NaN values for numerical columns with zeros (or None for nullable fields)\n",
    "update_data[[\"Daily_Variation\", \"Cumulative_Variation_10\", \"MA_10days\", \"MA_50days\", \"MA_200days\"]] = \\\n",
    "    update_data[[\"Daily_Variation\", \"Cumulative_Variation_10\", \"MA_10days\", \"MA_50days\", \"MA_200days\"]].fillna(0)\n",
    "\n",
    "# For Trend_Label, if NaN values exist, fill with 'Unknown'\n",
    "update_data[\"Trend_Label\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# Now continue with the rest of the code...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values in 'Month' with 0 or any other valid integer\n",
    "update_data[\"Month\"] = update_data[\"Month\"].fillna(0).astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date                        object\n",
      "Ticker                      object\n",
      "Company                     object\n",
      "Open_Price                 float64\n",
      "High_Price                 float64\n",
      "Low_Price                  float64\n",
      "Close_Price                float64\n",
      "Volume                       int64\n",
      "Daily_Variation            float64\n",
      "Cumulative_Variation_10    float64\n",
      "MA_10days                  float64\n",
      "MA_50days                  float64\n",
      "MA_200days                 float64\n",
      "Year                         int64\n",
      "Month                      float64\n",
      "Pct_Change_Daily           float64\n",
      "Pct_Change_Month           float64\n",
      "Pct_Change_Year            float64\n",
      "Trend_Label                 object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check Datatype\n",
    "\n",
    "# Check data types of all columns in the DataFrame\n",
    "print(update_data.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cleid\\AppData\\Local\\Temp\\ipykernel_17052\\353439332.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  update_data[\"Trend_Label\"].fillna(\"Unknown\", inplace=True)\n"
     ]
    },
    {
     "ename": "ProgrammingError",
     "evalue": "('42000', '[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]The incoming tabular data stream (TDS) remote procedure call (RPC) protocol stream is incorrect. Parameter 16 (\"\"): The supplied value is not a valid instance of data type float. Check the source data for invalid values. An example of an invalid value is data of numeric type with scale greater than precision. (8023) (SQLExecute)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m cursor\u001b[38;5;241m.\u001b[39mfast_executemany \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Execute bulk insert\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutemany\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_tuples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Commit and close\u001b[39;00m\n\u001b[0;32m     72\u001b[0m conn\u001b[38;5;241m.\u001b[39mcommit()\n",
      "\u001b[1;31mProgrammingError\u001b[0m: ('42000', '[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]The incoming tabular data stream (TDS) remote procedure call (RPC) protocol stream is incorrect. Parameter 16 (\"\"): The supplied value is not a valid instance of data type float. Check the source data for invalid values. An example of an invalid value is data of numeric type with scale greater than precision. (8023) (SQLExecute)')"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Establish connection\n",
    "conn = pyodbc.connect(\n",
    "    'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "    'SERVER=localhost\\\\SQLExpress08;'  # Adjust server and instance if needed\n",
    "    'DATABASE=StockData;'              # Ensure this is the correct database\n",
    "    'Trusted_Connection=yes;'\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Load new data from your Update.csv\n",
    "update_data = pd.read_csv(r\"D:\\Project\\TradeVision\\Data\\Final Dataset\\Final_Dataset.csv\")\n",
    "\n",
    "# Ensure column names match SQL table\n",
    "expected_columns = [\n",
    "    \"Date\", \"Ticker\", \"Company\", \"Open_Price\", \"High_Price\", \"Low_Price\", \"Close_Price\",\n",
    "    \"Volume\", \"Daily_Variation\", \"Cumulative_Variation_10\", \"MA_10days\", \"MA_50days\",\n",
    "    \"MA_200days\", \"Year\", \"Month\", \"Pct_Change_Daily\", \"Pct_Change_Month\",\n",
    "    \"Pct_Change_Year\", \"Trend_Label\"\n",
    "]\n",
    "\n",
    "# Select only the expected columns\n",
    "update_data = update_data[expected_columns]\n",
    "\n",
    "# Fill NaN values in the 'Year' column with a placeholder (e.g., 0)\n",
    "update_data[\"Year\"] = update_data[\"Year\"].fillna(0).astype(int)\n",
    "\n",
    "# Convert 'Month' to an integer (extract the month)\n",
    "update_data[\"Month\"] = pd.to_datetime(update_data[\"Month\"], errors='coerce').dt.month\n",
    "\n",
    "# Fill NaN values for numerical columns with zeros (or None for nullable fields)\n",
    "update_data[[\"Daily_Variation\", \"Cumulative_Variation_10\", \"MA_10days\", \"MA_50days\", \"MA_200days\"]] = \\\n",
    "    update_data[[\"Daily_Variation\", \"Cumulative_Variation_10\", \"MA_10days\", \"MA_50days\", \"MA_200days\"]].fillna(0)\n",
    "\n",
    "# For Trend_Label, if NaN values exist, fill with 'Unknown'\n",
    "update_data[\"Trend_Label\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# Convert Date column to correct format (if needed)\n",
    "update_data[\"Date\"] = pd.to_datetime(update_data[\"Date\"]).dt.date\n",
    "\n",
    "# Ensure float columns do not have invalid values, convert invalid entries to None\n",
    "float_columns = [\"Open_Price\", \"High_Price\", \"Low_Price\", \"Close_Price\", \"Daily_Variation\", \n",
    "                 \"Cumulative_Variation_10\", \"MA_10days\", \"MA_50days\", \"MA_200days\", \n",
    "                 \"Pct_Change_Daily\", \"Pct_Change_Month\", \"Pct_Change_Year\"]\n",
    "\n",
    "# Replace any invalid values (e.g., NaN, infinity) with None (null values)\n",
    "update_data[float_columns] = update_data[float_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert DataFrame to list of tuples for bulk insert\n",
    "data_tuples = [tuple(x) for x in update_data.to_numpy()]\n",
    "\n",
    "# SQL Query with correct table name (StockData)\n",
    "sql = \"\"\"\n",
    "    INSERT INTO StockData (\n",
    "        Date, Ticker, Company, Open_Price, High_Price, Low_Price, Close_Price,\n",
    "        Volume, Daily_Variation, Cumulative_Variation_10, MA_10days, MA_50days,\n",
    "        MA_200days, Year, Month, Pct_Change_Daily, Pct_Change_Month, \n",
    "        Pct_Change_Year, Trend_Label\n",
    "    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "\"\"\"\n",
    "\n",
    "# Enable fast execution for bulk insert\n",
    "cursor.fast_executemany = True\n",
    "\n",
    "# Execute bulk insert\n",
    "cursor.executemany(sql, data_tuples)\n",
    "\n",
    "# Commit and close\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"✅ Bulk insert completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
