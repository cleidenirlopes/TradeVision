{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING MODEL -  IROHACK PROJECT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå STEP 1: Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from tabulate import tabulate\n",
    "from datetime import date, timedelta\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import seaborn as sns\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå STEP 1.1: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "| Dataset   | Shape        |\n",
      "+===========+==============+\n",
      "| X shape   | (249882, 10) |\n",
      "+-----------+--------------+\n",
      "| y shape   | (249882,)    |\n",
      "+-----------+--------------+\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "Final_Dataset = pd.read_csv(r\"D:\\Project\\TradeVision\\Data\\Final Dataset\\Final_Dataset.csv\")\n",
    "\n",
    "# Define the feature columns (excluding categorical columns like Date, Ticker, and Company)\n",
    "feature_columns = [\n",
    "    'Open_Price', 'High_Price', 'Low_Price', 'Close_Price', 'Volume',\n",
    "    'Daily_Variation', 'Cumulative_Variation_10', 'MA_10days', 'MA_50days', 'MA_200days'\n",
    "]\n",
    "\n",
    "# Define the target variable\n",
    "target_column = 'Trend_Label'\n",
    "\n",
    "# Extract features (X) and target (y)\n",
    "X = Final_Dataset[feature_columns].values  # Convert to NumPy array\n",
    "y = Final_Dataset[target_column].values  # Convert to NumPy array\n",
    "\n",
    "# Print shapes using tabulate\n",
    "table = [[\"X shape\", X.shape], [\"y shape\", y.shape]]\n",
    "print(tabulate(table, headers=[\"Dataset\", \"Shape\"], tablefmt=\"grid\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå STEP 1.2: Time-Series Aware Train-Test Split Completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+-----------+\n",
      "| Dataset         |   Rows |   Columns |\n",
      "+=================+========+===========+\n",
      "| Train Data      | 167814 |        31 |\n",
      "+-----------------+--------+-----------+\n",
      "| Validation Data |  36380 |        31 |\n",
      "+-----------------+--------+-----------+\n",
      "| Test Data       |  45688 |        31 |\n",
      "+-----------------+--------+-----------+\n",
      "‚úÖ Train, Validation, and Test splits are saved!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load dataset\n",
    "file_path = r\"D:\\Project\\TradeVision\\Data\\Final Dataset\\Final_Dataset_Features.csv\"\n",
    "Final_Dataset = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the dataset is sorted by Date\n",
    "Final_Dataset['Date'] = pd.to_datetime(Final_Dataset['Date'], errors='coerce')\n",
    "Final_Dataset.sort_values(by=['Date', 'Ticker'], inplace=True)\n",
    "\n",
    "# üìå **1Ô∏è‚É£ Define Train, Validation, Test Split (Time-Series Aware)**\n",
    "train_size = 0.7\n",
    "val_size = 0.15  # 15% for validation\n",
    "test_size = 0.15  # 15% for testing\n",
    "\n",
    "# Get unique dates in order\n",
    "unique_dates = Final_Dataset['Date'].unique()\n",
    "num_dates = len(unique_dates)\n",
    "\n",
    "# Compute split indices\n",
    "train_end = int(train_size * num_dates)\n",
    "val_end = int((train_size + val_size) * num_dates)\n",
    "\n",
    "# Define train, validation, and test sets\n",
    "train_dates = unique_dates[:train_end]\n",
    "val_dates = unique_dates[train_end:val_end]\n",
    "test_dates = unique_dates[val_end:]\n",
    "\n",
    "train_data = Final_Dataset[Final_Dataset['Date'].isin(train_dates)]\n",
    "val_data = Final_Dataset[Final_Dataset['Date'].isin(val_dates)]\n",
    "test_data = Final_Dataset[Final_Dataset['Date'].isin(test_dates)]\n",
    "\n",
    "# üìå **Print dataset sizes in tabular format**\n",
    "table_data = [\n",
    "    [\"Train Data\", train_data.shape[0], train_data.shape[1]],\n",
    "    [\"Validation Data\", val_data.shape[0], val_data.shape[1]],\n",
    "    [\"Test Data\", test_data.shape[0], test_data.shape[1]],\n",
    "]\n",
    "\n",
    "print(tabulate(table_data, headers=[\"Dataset\", \"Rows\", \"Columns\"], tablefmt=\"grid\"))\n",
    "\n",
    "# Save the datasets separately\n",
    "train_data.to_csv(r\"D:\\Project\\TradeVision\\Data\\Train_Dataset.csv\", index=False)\n",
    "val_data.to_csv(r\"D:\\Project\\TradeVision\\Data\\Validation_Dataset.csv\", index=False)\n",
    "test_data.to_csv(r\"D:\\Project\\TradeVision\\Data\\Test_Dataset.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Train, Validation, and Test splits are saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå STEP 1.3: Encoding the Target Variable (Trend_Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cleid\\AppData\\Local\\Temp\\ipykernel_20668\\1381418077.py:10: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test = pd.read_csv(r\"D:\\Project\\TradeVision\\Data\\Splits\\Test_Dataset.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Scaled & Encoded Train Data:\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+-------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|    | Date       | Ticker   | Company                    | Sector                 | Country       |   Open_Price |   High_Price |   Low_Price |   Close_Price |      Volume |   Daily_Variation |   Cumulative_Variation_10 |   MA_10days |   MA_50days |   MA_200days | Month   |   Pct_Change_Month |   Year |   Pct_Change_Year |   Pct_Change_Daily |   Trend_Label |   Close_Price_t-1 |   Close_Price_t-5 |   Close_Price_t-10 |   RSI_14 |   MACD_Line |   MACD_Signal |   MACD_Histogram |   Bollinger_MA |   Bollinger_Upper |   Bollinger_Lower |\n",
      "+====+============+==========+============================+========================+===============+==============+==============+=============+===============+=============+===================+===========================+=============+=============+==============+=========+====================+========+===================+====================+===============+===================+===================+====================+==========+=============+===============+==================+================+===================+===================+\n",
      "|  0 | 2023-08-11 | A        | Agilent Technologies, Inc. | Health Care            | United States |   0.0145948  |   0.0144644  |  0.0147631  |    0.0147032  | 0.00109178  |          0.420237 |                  0.428848 |  0.0148482  |  0.0160534  |   0.0222158  | 2023-08 |           0.625688 |    0.6 |          0.420237 |           0.420237 |             1 |        0.0119185  |        0.0102388  |         0.0133729  | 0.544422 |    0.127841 |     0.061292  |         0.43289  |     0.0142618  |        0.0141798  |         0.0273187 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+-------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  1 | 2023-08-11 | AAPL     | Apple Inc.                 | Information Technology | United States |   0.0208488  |   0.0206629  |  0.0210672  |    0.0210605  | 0.045505    |          0.425459 |                  0.374916 |  0.0222738  |  0.0219126  |   0.0257488  | 2023-08 |           0.633463 |    0.6 |          0.425459 |           0.425459 |             0 |        0.0193622  |        0.0181032  |         0.0158111  | 0.490892 |    0.130039 |     0.0607127 |         0.438186 |     0.0190574  |        0.018624   |         0.0323494 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+-------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  2 | 2023-08-11 | ABBV     | AbbVie Inc.                | Health Care            | United States |   0.016863   |   0.0167536  |  0.0170902  |    0.0170079  | 0.00301613  |          0.429098 |                  0.456457 |  0.0168562  |  0.0174366  |   0.0210063  | 2023-08 |           0.638882 |    0.6 |          0.429098 |           0.429098 |             0 |        0.0129558  |        0.0127588  |         0.0147139  | 0.542406 |    0.128464 |     0.0615312 |         0.433773 |     0.0156513  |        0.0138127  |         0.0312868 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+-------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  3 | 2023-08-11 | ABT      | Abbott Laboratories        | Health Care            | United States |   0.011586   |   0.0115111  |  0.0116967  |    0.0117882  | 0.00303696  |          0.429543 |                  0.391808 |  0.0122065  |  0.0125606  |   0.0172518  | 2023-08 |           0.639544 |    0.6 |          0.429543 |           0.429543 |             0 |        0.0104716  |        0.00863146 |         0.00966175 | 0.555126 |    0.12913  |     0.0614758 |         0.435194 |     0.0110396  |        0.00985347 |         0.0259719 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+-------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  4 | 2023-08-11 | ACGL     | Arch Capital Group         | Financials             | United States |   0.00808853 |   0.00794774 |  0.00816352 |    0.00813339 | 0.000891654 |          0.425398 |                  0.402337 |  0.00830862 |  0.00818771 |   0.00745757 | 2023-08 |           0.633372 |    0.6 |          0.425398 |           0.425398 |             0 |        0.00707616 |        0.00762795 |         0.00619031 | 0.454729 |    0.128873 |     0.0613908 |         0.434809 |     0.00743133 |        0.00711149 |         0.0212736 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+-------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "\n",
      "Sample Scaled & Encoded Validation Data:\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|    | Date       | Ticker   | Company                    | Sector                 | Country       |   Open_Price |   High_Price |   Low_Price |   Close_Price |     Volume |   Daily_Variation |   Cumulative_Variation_10 |   MA_10days |   MA_50days |   MA_200days | Month   |   Pct_Change_Month |   Year |   Pct_Change_Year |   Pct_Change_Daily |   Trend_Label |   Close_Price_t-1 |   Close_Price_t-5 |   Close_Price_t-10 |   RSI_14 |   MACD_Line |   MACD_Signal |   MACD_Histogram |   Bollinger_MA |   Bollinger_Upper |   Bollinger_Lower |\n",
      "+====+============+==========+============================+========================+===============+==============+==============+=============+===============+============+===================+===========================+=============+=============+==============+=========+====================+========+===================+====================+===============+===================+===================+====================+==========+=============+===============+==================+================+===================+===================+\n",
      "|  0 | 2020-01-02 | A        | Agilent Technologies, Inc. | Health Care            | United States |   0.00935193 |   0.0092201  |  0.00940875 |    0.00940824 | 0.00122643 |               nan |                       nan |  0.00956548 |  0.0100286  |    0.012924  | 2020-01 |                nan |      0 |               nan |                nan |             2 |        0.0113782  |        0.00817212 |                nan | 0.604813 |    0.138042 |     0.0687246 |         0.441981 |     0.00953051 |        0.0104     |         0.0214382 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  1 | 2020-01-02 | AAPL     | Apple Inc.                 | Information Technology | United States |   0.00795624 |   0.00791929 |  0.00804507 |    0.00811808 | 0.118599   |               nan |                       nan |  0.00824742 |  0.0086318  |    0.0111239 | 2020-01 |                nan |      0 |               nan |                nan |             2 |        0.0122026  |        0.00728281 |                nan | 0.550017 |    0.142825 |     0.0734739 |         0.444306 |     0.00997797 |        0.0130441  |         0.0185254 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  2 | 2020-01-02 | ABBV     | AbbVie Inc.                | Health Care            | United States |   0.00789358 |   0.00777656 |  0.00795619 |    0.00797762 | 0.00492848 |               nan |                       nan |  0.00810393 |  0.00847973 |    0.010928  | 2020-01 |                nan |      0 |               nan |                nan |             2 |        0.00875196 |        0.00690521 |                nan | 0.59687  |    0.134524 |     0.0662877 |         0.438653 |     0.00786926 |        0.00783194 |         0.0212557 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  3 | 2020-01-02 | ABT      | Abbott Laboratories        | Health Care            | United States |   0.00878949 |   0.00871241 |  0.00889299 |    0.00893591 | 0.00434175 |               nan |                       nan |  0.00908295 |  0.00951725 |    0.012265  | 2020-01 |                nan |      0 |               nan |                nan |             2 |        0.0101168  |        0.00801112 |                nan | 0.584752 |    0.135719 |     0.0674964 |         0.4392   |     0.00949786 |        0.00964587 |         0.022502  |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  4 | 2020-01-02 | ACGL     | Arch Capital Group         | Financials             | United States |   0.00414654 |   0.00407858 |  0.00419455 |    0.00419634 | 0.00119684 |               nan |                       nan |  0.00424086 |  0.00438583 |    0.0056522 | 2020-01 |                nan |      0 |               nan |                nan |             2 |        0.00269002 |        0.00215974 |                nan | 0.794576 |    0.131082 |     0.0623725 |         0.437739 |     0.00245554 |        0.00291705 |         0.0154217 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "\n",
      "Sample Scaled & Encoded Test Data:\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|    | Date       | Ticker   | Company                    | Sector                 | Country       |   Open_Price |   High_Price |   Low_Price |   Close_Price |     Volume |   Daily_Variation |   Cumulative_Variation_10 |   MA_10days |   MA_50days |   MA_200days | Month   |   Pct_Change_Month |   Year |   Pct_Change_Year |   Pct_Change_Daily |   Trend_Label |   Close_Price_t-1 |   Close_Price_t-5 |   Close_Price_t-10 |   RSI_14 |   MACD_Line |   MACD_Signal |   MACD_Histogram |   Bollinger_MA |   Bollinger_Upper |   Bollinger_Lower |\n",
      "+====+============+==========+============================+========================+===============+==============+==============+=============+===============+============+===================+===========================+=============+=============+==============+=========+====================+========+===================+====================+===============+===================+===================+====================+==========+=============+===============+==================+================+===================+===================+\n",
      "|  0 | 2024-06-05 | A        | Agilent Technologies, Inc. | Health Care            | United States |    0.0151411 |    0.0153203 |   0.0153036 |     0.015618  | 0.0023619  |          0.44176  |                  0.385656 |   0.0166289 |   0.0173399 |    0.0212433 | 2024-06 |           0.657734 |    0.8 |          0.44176  |           0.44176  |             0 |        0.0147281  |        0.0135162  |         0.0138069  | 0.438271 |    0.129357 |     0.0629671 |         0.433368 |      0.0154634 |         0.0134555 |         0.0313674 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  1 | 2024-06-05 | AAPL     | Apple Inc.                 | Information Technology | United States |    0.023162  |    0.0229693 |   0.0234448 |     0.0233947 | 0.0474036  |          0.431447 |                  0.464446 |   0.0225206 |   0.0231313 |    0.0272351 | 2024-06 |           0.642378 |    0.8 |          0.431447 |           0.431447 |             0 |        0.0174616  |        0.023915   |         0.0165812  | 0.448193 |    0.125338 |     0.0635904 |         0.424352 |      0.0228966 |         0.0233546 |         0.0345978 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  2 | 2024-06-05 | ABBV     | AbbVie Inc.                | Health Care            | United States |    0.0186997 |    0.0187518 |   0.0188913 |     0.0191424 | 0.00445687 |          0.441474 |                  0.408377 |   0.019074  |   0.020624  |    0.0233617 | 2024-06 |           0.657308 |    0.8 |          0.441474 |           0.441474 |             1 |        0.01695    |        0.0170435  |         0.0159716  | 0.416981 |    0.128761 |     0.0655428 |         0.428232 |      0.0195961 |         0.0185553 |         0.0337762 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  3 | 2024-06-05 | ABT      | Abbott Laboratories        | Health Care            | United States |    0.0116214 |    0.0114641 |   0.0116798 |     0.0117289 | 0.00288262 |          0.423642 |                  0.385424 |   0.0121272 |   0.0135198 |    0.0163949 | 2024-06 |           0.630758 |    0.8 |          0.423642 |           0.423642 |             1 |        0.0110315  |        0.0110302  |         0.0105017  | 0.445874 |    0.128864 |     0.0631681 |         0.432071 |      0.0121448 |         0.010703  |         0.0273962 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  4 | 2024-06-05 | ACGL     | Arch Capital Group         | Financials             | United States |    0.0109116 |    0.010769  |   0.0108847 |     0.0108704 | 0.00128736 |          0.415828 |                  0.415934 |   0.0109257 |   0.0100756 |    0.0101288 | 2024-06 |           0.619124 |    0.8 |          0.415828 |           0.415828 |             0 |        0.00901669 |        0.00932229 |         0.00834824 | 0.456339 |    0.128548 |     0.0631101 |         0.431526 |      0.0100682 |         0.0100659 |         0.0232649 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "‚úÖ Feature scaling and encoding completed! Processed datasets saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import joblib  # To save the scaler and encoder\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load the train, validation, and test sets\n",
    "train = pd.read_csv(r\"D:\\Project\\TradeVision\\Data\\Splits\\Validation_Dataset.csv\")\n",
    "val = pd.read_csv(r\"D:\\Project\\TradeVision\\Data\\Splits\\Train_Dataset.csv\")\n",
    "test = pd.read_csv(r\"D:\\Project\\TradeVision\\Data\\Splits\\Test_Dataset.csv\")\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_features = train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform only the numerical columns\n",
    "train[numerical_features] = scaler.fit_transform(train[numerical_features])\n",
    "val[numerical_features] = scaler.transform(val[numerical_features])\n",
    "test[numerical_features] = scaler.transform(test[numerical_features])\n",
    "\n",
    "\n",
    "# Save the scaler for later use\n",
    "scaler_path = \"D:/Project/TradeVision/Data/Scalers/minmax_scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "# Encode Trend_Label using Label Encoding\n",
    "encoder = LabelEncoder()\n",
    "train[\"Trend_Label\"] = encoder.fit_transform(train[\"Trend_Label\"])\n",
    "val[\"Trend_Label\"] = encoder.transform(val[\"Trend_Label\"])\n",
    "test[\"Trend_Label\"] = encoder.transform(test[\"Trend_Label\"])\n",
    "\n",
    "# Save the encoder for later use\n",
    "encoder_path = \"D:/Project/TradeVision/Data/Scalers/label_encoder.pkl\"\n",
    "joblib.dump(encoder, encoder_path)\n",
    "\n",
    "# Save the processed datasets\n",
    "train.to_csv(r\"D:\\Project\\TradeVision\\Data\\Splits\\Validation_Dataset.csv\", index=False)\n",
    "val.to_csv(r\"D:\\Project\\TradeVision\\Data\\Splits\\Train_Dataset.csv\", index=False)\n",
    "test.to_csv(r\"D:\\Project\\TradeVision\\Data\\Splits\\Test_Dataset.csv\", index=False)\n",
    "\n",
    "# Print sample data using tabulate\n",
    "print(\"\\nSample Scaled & Encoded Train Data:\")\n",
    "print(tabulate(train.head(), headers='keys', tablefmt='grid'))\n",
    "\n",
    "print(\"\\nSample Scaled & Encoded Validation Data:\")\n",
    "print(tabulate(val.head(), headers='keys', tablefmt='grid'))\n",
    "\n",
    "print(\"\\nSample Scaled & Encoded Test Data:\")\n",
    "print(tabulate(test.head(), headers='keys', tablefmt='grid'))\n",
    "\n",
    "print(\"‚úÖ Feature scaling and encoding completed! Processed datasets saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå STEP 1.4: Data Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cleid\\AppData\\Local\\Temp\\ipykernel_23348\\3865369082.py:10: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test = pd.read_csv(r\"D:\\Project\\TradeVision\\Data\\Splits\\Test_Dataset.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Scaled Train Data:\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+-------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|    | Date       | Ticker   | Company                    | Sector                 | Country       |   Open_Price |   High_Price |   Low_Price |   Close_Price |      Volume |   Daily_Variation |   Cumulative_Variation_10 |   MA_10days |   MA_50days |   MA_200days | Month   |   Pct_Change_Month |   Year |   Pct_Change_Year |   Pct_Change_Daily |   Trend_Label |   Close_Price_t-1 |   Close_Price_t-5 |   Close_Price_t-10 |   RSI_14 |   MACD_Line |   MACD_Signal |   MACD_Histogram |   Bollinger_MA |   Bollinger_Upper |   Bollinger_Lower |\n",
      "+====+============+==========+============================+========================+===============+==============+==============+=============+===============+=============+===================+===========================+=============+=============+==============+=========+====================+========+===================+====================+===============+===================+===================+====================+==========+=============+===============+==================+================+===================+===================+\n",
      "|  0 | 2020-01-02 | A        | Agilent Technologies, Inc. | Health Care            | United States |   0.012779   |   0.0127305  |  0.0127539  |    0.0126913  | 0.000963664 |               nan |                       nan |  0.0128612  |  0.0139636  |   0.0161064  | 2020-01 |                nan |      0 |               nan |                nan |             1 |        0.0168637  |        0.0130231  |                nan | 0.604813 |    0.255463 |      0.223014 |         0.367995 |     0.0145656  |        0.0165935  |         0.027287  |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+-------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  1 | 2020-01-02 | AAPL     | Apple Inc.                 | Information Technology | United States |   0.0109968  |   0.0110605  |  0.0110294  |    0.0110713  | 0.0925612   |               nan |                       nan |  0.0112175  |  0.0121743  |   0.0140425  | 2020-01 |                nan |      0 |               nan |                nan |             1 |        0.0180372  |        0.0116776  |                nan | 0.550017 |    0.260982 |      0.228875 |         0.370856 |     0.0151978  |        0.0205151  |         0.0234081 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+-------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  2 | 2020-01-02 | ABBV     | AbbVie Inc.                | Health Care            | United States |   0.0109168  |   0.0108773  |  0.010917   |    0.0108949  | 0.00385274  |               nan |                       nan |  0.0110386  |  0.0119795  |   0.0138178  | 2020-01 |                nan |      0 |               nan |                nan |             1 |        0.0131255  |        0.0111062  |                nan | 0.59687  |    0.251402 |      0.220007 |         0.363902 |     0.0122187  |        0.0127848  |         0.027044  |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+-------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  3 | 2020-01-02 | ABT      | Abbott Laboratories        | Health Care            | United States |   0.0120608  |   0.0120787  |  0.0121017  |    0.0120982  | 0.00339486  |               nan |                       nan |  0.0122594  |  0.0133086  |   0.0153509  | 2020-01 |                nan |      0 |               nan |                nan |             1 |        0.0150682  |        0.0127795  |                nan | 0.584752 |    0.252782 |      0.221499 |         0.364575 |     0.0145195  |        0.0154751  |         0.0287036 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+-------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  4 | 2020-01-02 | ACGL     | Arch Capital Group         | Financials             | United States |   0.00613201 |   0.00612989 |  0.00615996 |    0.00614668 | 0.000940572 |               nan |                       nan |  0.00622123 |  0.00673518 |   0.00776875 | 2020-01 |                nan |      0 |               nan |                nan |             1 |        0.00449667 |        0.00392618 |                nan | 0.794576 |    0.24743  |      0.215176 |         0.362779 |     0.00457035 |        0.00549537 |         0.0192751 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+-------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "\n",
      "Sample Scaled Validation Data:\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+-------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|    | Date       | Ticker   | Company                    | Sector                 | Country       |   Open_Price |   High_Price |   Low_Price |   Close_Price |      Volume |   Daily_Variation |   Cumulative_Variation_10 |   MA_10days |   MA_50days |   MA_200days | Month   |   Pct_Change_Month |   Year |   Pct_Change_Year |   Pct_Change_Daily |   Trend_Label |   Close_Price_t-1 |   Close_Price_t-5 |   Close_Price_t-10 |   RSI_14 |   MACD_Line |   MACD_Signal |   MACD_Histogram |   Bollinger_MA |   Bollinger_Upper |   Bollinger_Lower |\n",
      "+====+============+==========+============================+========================+===============+==============+==============+=============+===============+=============+===================+===========================+=============+=============+==============+=========+====================+========+===================+====================+===============+===================+===================+====================+==========+=============+===============+==================+================+===================+===================+\n",
      "|  0 | 2023-08-11 | A        | Agilent Technologies, Inc. | Health Care            | United States |    0.0194738 |    0.019463  |   0.0195251 |     0.0193403 | 0.000858587 |          0.409811 |                  0.582589 |   0.0194488 |   0.0216814 |   0.0267602  | 2023-08 |           0.579041 |    0.6 |          0.409811 |           0.409811 |           0.5 |         0.0176328 |         0.0161501 |         0.020552   | 0.544422 |    0.243688 |      0.213842 |         0.356814 |      0.0212498 |         0.0221995 |         0.0351178 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+-------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  1 | 2023-08-11 | AAPL     | Apple Inc.                 | Information Technology | United States |    0.0274598 |    0.0274206 |   0.0274974 |     0.0273232 | 0.0355187   |          0.414834 |                  0.5366   |   0.0287087 |   0.029187  |   0.0308111  | 2023-08 |           0.586138 |    0.6 |          0.414834 |           0.414834 |           0   |         0.0282286 |         0.0280491 |         0.024242   | 0.490892 |    0.246225 |      0.213128 |         0.363328 |      0.0280249 |         0.0287908 |         0.041817  |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+-------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  2 | 2023-08-11 | ABBV     | AbbVie Inc.                | Health Care            | United States |    0.0223702 |    0.0224018 |   0.022468  |     0.0222343 | 0.00236035  |          0.418335 |                  0.606131 |   0.0219528 |   0.0234532 |   0.0253734  | 2023-08 |           0.591085 |    0.6 |          0.418335 |           0.418335 |           0   |         0.0191094 |         0.019963  |         0.0225814  | 0.542406 |    0.244407 |      0.214138 |         0.3579   |      0.0232129 |         0.0216549 |         0.040402  |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+-------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  3 | 2023-08-11 | ABT      | Abbott Laboratories        | Health Care            | United States |    0.0156317 |    0.0156716 |   0.0156473 |     0.0156798 | 0.00237661  |          0.418763 |                  0.551004 |   0.0161545 |   0.0172071 |   0.0210686  | 2023-08 |           0.591689 |    0.6 |          0.418763 |           0.418763 |           0   |         0.0155732 |         0.0137181 |         0.0149356  | 0.555126 |    0.245176 |      0.214069 |         0.359648 |      0.0166977 |         0.015783  |         0.0333244 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+-------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  4 | 2023-08-11 | ACGL     | Arch Capital Group         | Financials             | United States |    0.0111657 |    0.0110971 |   0.0111792 |     0.0110905 | 0.000702406 |          0.414775 |                  0.559982 |   0.0112938 |   0.0116054 |   0.00983875 | 2023-08 |           0.586055 |    0.6 |          0.414775 |           0.414775 |           0   |         0.0107401 |         0.0121998 |         0.00968204 | 0.454729 |    0.244879 |      0.213964 |         0.359174 |      0.0116    |         0.0117163 |         0.0270678 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+-------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "\n",
      "Sample Scaled Test Data:\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|    | Date       | Ticker   | Company                    | Sector                 | Country       |   Open_Price |   High_Price |   Low_Price |   Close_Price |     Volume |   Daily_Variation |   Cumulative_Variation_10 |   MA_10days |   MA_50days |   MA_200days | Month   |   Pct_Change_Month |   Year |   Pct_Change_Year |   Pct_Change_Daily |   Trend_Label |   Close_Price_t-1 |   Close_Price_t-5 |   Close_Price_t-10 |   RSI_14 |   MACD_Line |   MACD_Signal |   MACD_Histogram |   Bollinger_MA |   Bollinger_Upper |   Bollinger_Lower |\n",
      "+====+============+==========+============================+========================+===============+==============+==============+=============+===============+============+===================+===========================+=============+=============+==============+=========+====================+========+===================+====================+===============+===================+===================+====================+==========+=============+===============+==================+================+===================+===================+\n",
      "|  0 | 2024-06-05 | A        | Agilent Technologies, Inc. | Health Care            | United States |    0.0201714 |    0.0205619 |   0.0202086 |     0.020489  | 0.00184978 |          0.430514 |                  0.545758 |   0.0216694 |   0.0233295 |    0.0256452 | 2024-06 |           0.608294 |    0.8 |          0.430514 |           0.430514 |           0   |         0.0216321 |         0.021109  |          0.0212089 | 0.438271 |    0.245438 |      0.21591  |         0.357402 |      0.0229475 |         0.0211252 |         0.0405094 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  1 | 2024-06-05 | AAPL     | Apple Inc.                 | Information Technology | United States |    0.0304136 |    0.0303815 |   0.0305042 |     0.0302544 | 0.0370003  |          0.420594 |                  0.612944 |   0.0290165 |   0.0307482 |    0.0325153 | 2024-06 |           0.594276 |    0.8 |          0.420594 |           0.420594 |           0   |         0.0255232 |         0.0368426 |          0.0254074 | 0.448193 |    0.2408   |      0.216679 |         0.346312 |      0.0334488 |         0.0358069 |         0.0448112 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  2 | 2024-06-05 | ABBV     | AbbVie Inc.                | Health Care            | United States |    0.0247155 |    0.0249671 |   0.0247457 |     0.0249146 | 0.0034847  |          0.430239 |                  0.565133 |   0.0247185 |   0.0275364 |    0.0280741 | 2024-06 |           0.607904 |    0.8 |          0.430239 |           0.430239 |           0.5 |         0.0247949 |         0.0264458 |          0.0244848 | 0.416981 |    0.244751 |      0.219088 |         0.351084 |      0.028786  |         0.0286888 |         0.043717  |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  3 | 2024-06-05 | ABT      | Abbott Laboratories        | Health Care            | United States |    0.015677  |    0.0156114 |   0.0156259 |     0.0156054 | 0.00225616 |          0.413086 |                  0.54556  |   0.0160557 |   0.0184358 |    0.0200861 | 2024-06 |           0.583668 |    0.8 |          0.413086 |           0.413086 |           0.5 |         0.0163702 |         0.0173475 |          0.0162068 | 0.445874 |    0.244869 |      0.216158 |         0.355807 |      0.018259  |         0.0170429 |         0.035221  |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "|  4 | 2024-06-05 | ACGL     | Arch Capital Group         | Financials             | United States |    0.0147706 |    0.014719  |   0.0146204 |     0.0145273 | 0.00101122 |          0.40557  |                  0.571576 |   0.0145574 |   0.0140239 |    0.0129015 | 2024-06 |           0.573048 |    0.8 |          0.40557  |           0.40557  |           0   |         0.0135023 |         0.0147634 |          0.0129478 | 0.456339 |    0.244505 |      0.216086 |         0.355137 |      0.0153253 |         0.016098  |         0.0297195 |\n",
      "+----+------------+----------+----------------------------+------------------------+---------------+--------------+--------------+-------------+---------------+------------+-------------------+---------------------------+-------------+-------------+--------------+---------+--------------------+--------+-------------------+--------------------+---------------+-------------------+-------------------+--------------------+----------+-------------+---------------+------------------+----------------+-------------------+-------------------+\n",
      "‚úÖ Feature scaling completed! Scaled datasets saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib  # To save the scaler\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load the train, validation, and test sets\n",
    "train = pd.read_csv(r\"D:\\Project\\TradeVision\\Data\\Splits\\Train_Dataset.csv\")\n",
    "val = pd.read_csv(r\"D:\\Project\\TradeVision\\Data\\Splits\\Validation_Dataset.csv\")\n",
    "test = pd.read_csv(r\"D:\\Project\\TradeVision\\Data\\Splits\\Test_Dataset.csv\")\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_features = train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform only the numerical columns\n",
    "train[numerical_features] = scaler.fit_transform(train[numerical_features])\n",
    "val[numerical_features] = scaler.transform(val[numerical_features])\n",
    "test[numerical_features] = scaler.transform(test[numerical_features])\n",
    "\n",
    "# Save the scaler for later use\n",
    "scaler_path = \"D:/Project/TradeVision/Data/Scalers/minmax_scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "# Save the normalized datasets\n",
    "train.to_csv(r\"D:\\Project\\TradeVision\\Data\\Splits\\Train_Dataset.csv\", index=False)\n",
    "val.to_csv(r\"D:\\Project\\TradeVision\\Data\\Splits\\Validation_Dataset.csv\", index=False)\n",
    "test.to_csv(r\"D:\\Project\\TradeVision\\Data\\Splits\\Test_Dataset.csv\", index=False)\n",
    "\n",
    "# Print sample data using tabulate\n",
    "print(\"\\nSample Scaled Train Data:\")\n",
    "print(tabulate(train.head(), headers='keys', tablefmt='grid'))\n",
    "\n",
    "print(\"\\nSample Scaled Validation Data:\")\n",
    "print(tabulate(val.head(), headers='keys', tablefmt='grid'))\n",
    "\n",
    "print(\"\\nSample Scaled Test Data:\")\n",
    "print(tabulate(test.head(), headers='keys', tablefmt='grid'))\n",
    "\n",
    "print(\"‚úÖ Feature scaling completed! Scaled datasets saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå STEP 1.5: Reshape Data for LSTM Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "| Dataset          | Shape            |\n",
      "+==================+==================+\n",
      "| X_train_reshaped | (36370, 10, 10)  |\n",
      "+------------------+------------------+\n",
      "| X_val_reshaped   | (167804, 10, 10) |\n",
      "+------------------+------------------+\n",
      "| X_test_reshaped  | (45678, 10, 10)  |\n",
      "+------------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Define the time_steps (e.g., 10 previous days to predict the next day's value)\n",
    "time_steps = 10\n",
    "\n",
    "# Extract the feature columns (excluding categorical columns like Date, Ticker, and Company)\n",
    "feature_columns = [\n",
    "    'Open_Price', 'High_Price', 'Low_Price', 'Close_Price', 'Volume',\n",
    "    'Daily_Variation', 'Cumulative_Variation_10', 'MA_10days', 'MA_50days', 'MA_200days'\n",
    "]\n",
    "\n",
    "# Function to reshape data into time-steps format\n",
    "def reshape_data(data, time_steps, feature_columns):\n",
    "    num_samples = len(data) - time_steps\n",
    "    reshaped_data = np.zeros((num_samples, time_steps, len(feature_columns)))\n",
    "    for i in range(time_steps, len(data)):\n",
    "        reshaped_data[i - time_steps] = data[feature_columns].iloc[i-time_steps:i].values\n",
    "    return reshaped_data\n",
    "\n",
    "# Reshape the train, validation, and test datasets into 3D arrays: (samples, time_steps, features)\n",
    "X_train_reshaped = reshape_data(train, time_steps, feature_columns)\n",
    "X_val_reshaped = reshape_data(val, time_steps, feature_columns)\n",
    "X_test_reshaped = reshape_data(test, time_steps, feature_columns)\n",
    "\n",
    "# Print the shapes in a tabular format\n",
    "shapes = [\n",
    "    [\"X_train_reshaped\", X_train_reshaped.shape],\n",
    "    [\"X_val_reshaped\", X_val_reshaped.shape],\n",
    "    [\"X_test_reshaped\", X_test_reshaped.shape]\n",
    "]\n",
    "\n",
    "print(tabulate(shapes, headers=[\"Dataset\", \"Shape\"], tablefmt=\"grid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå STEP 1.6: Build for Long Time Short Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------------------------------------------+\n",
      "| Layer                 | Details                                     |\n",
      "+=======================+=============================================+\n",
      "| LSTM Layer            | Input size: 10, Hidden size: 100, Layers: 2 |\n",
      "+-----------------------+---------------------------------------------+\n",
      "| Fully Connected Layer | Input features: 100, Output features: 3     |\n",
      "+-----------------------+---------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the LSTM model for classification\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through LSTM\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        # Use the last time step's output\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Model parameters\n",
    "input_size = X_train_reshaped.shape[2]  # Number of features\n",
    "hidden_size = 100  # Hidden units\n",
    "num_layers = 2  # LSTM layers\n",
    "output_size = 3  # Number of classes (Up, Down, Neutral)\n",
    "\n",
    "# Initialize model\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Print model summary\n",
    "model_summary = [\n",
    "    [\"Layer\", \"Details\"],\n",
    "    [\"LSTM Layer\", f\"Input size: {input_size}, Hidden size: {hidden_size}, Layers: {num_layers}\"],\n",
    "    [\"Fully Connected Layer\", f\"Input features: {hidden_size}, Output features: {output_size}\"]\n",
    "]\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate(model_summary, headers=\"firstrow\", tablefmt=\"grid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå STEP 1.6: Handling Missing Values in Stock Market Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final missing values:\n",
      " Date                       14896\n",
      "Ticker                         0\n",
      "Company                        0\n",
      "Sector                         0\n",
      "Country                        0\n",
      "Open_Price                     0\n",
      "High_Price                     0\n",
      "Low_Price                      0\n",
      "Close_Price                    0\n",
      "Volume                         0\n",
      "Daily_Variation               50\n",
      "Cumulative_Variation_10       50\n",
      "MA_10days                      0\n",
      "MA_50days                      0\n",
      "MA_200days                     0\n",
      "Month                          0\n",
      "Pct_Change_Month              50\n",
      "Year                           0\n",
      "Pct_Change_Year               50\n",
      "Pct_Change_Daily              50\n",
      "Trend_Label                    0\n",
      "Close_Price_t-1                0\n",
      "Close_Price_t-5                0\n",
      "Close_Price_t-10             462\n",
      "RSI_14                         0\n",
      "MACD_Line                      0\n",
      "MACD_Signal                    0\n",
      "MACD_Histogram                 0\n",
      "Bollinger_MA                   0\n",
      "Bollinger_Upper                0\n",
      "Bollinger_Lower                0\n",
      "dtype: int64\n",
      "Missing values after second imputation:\n",
      " Date                       14896\n",
      "Ticker                         0\n",
      "Company                        0\n",
      "Sector                         0\n",
      "Country                        0\n",
      "Open_Price                     0\n",
      "High_Price                     0\n",
      "Low_Price                      0\n",
      "Close_Price                    0\n",
      "Volume                         0\n",
      "Daily_Variation               50\n",
      "Cumulative_Variation_10       50\n",
      "MA_10days                      0\n",
      "MA_50days                      0\n",
      "MA_200days                     0\n",
      "Month                          0\n",
      "Pct_Change_Month              50\n",
      "Year                           0\n",
      "Pct_Change_Year               50\n",
      "Pct_Change_Daily              50\n",
      "Trend_Label                    0\n",
      "Close_Price_t-1                0\n",
      "Close_Price_t-5                0\n",
      "Close_Price_t-10             462\n",
      "RSI_14                         0\n",
      "MACD_Line                      0\n",
      "MACD_Signal                    0\n",
      "MACD_Histogram                 0\n",
      "Bollinger_MA                   0\n",
      "Bollinger_Upper                0\n",
      "Bollinger_Lower                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Forward fill for lagged close prices and percentage changes\n",
    "cols_ffill = ['Pct_Change_Month', 'Pct_Change_Year', 'Pct_Change_Daily', \n",
    "              'Close_Price_t-1', 'Close_Price_t-5', 'Close_Price_t-10']\n",
    "Final_Dataset[cols_ffill] = Final_Dataset[cols_ffill].ffill()\n",
    "\n",
    "# Mean imputation for Bollinger_Lower and other single missing value columns\n",
    "Final_Dataset['Bollinger_Lower'] = Final_Dataset['Bollinger_Lower'].fillna(Final_Dataset['Bollinger_Lower'].mean())\n",
    "\n",
    "# Interpolation for `Daily_Variation` & `Cumulative_Variation_10`\n",
    "Final_Dataset['Daily_Variation'] = Final_Dataset['Daily_Variation'].interpolate(method='linear')\n",
    "Final_Dataset['Cumulative_Variation_10'] = Final_Dataset['Cumulative_Variation_10'].interpolate(method='linear')\n",
    "\n",
    "# Forward fill the remaining missing values\n",
    "Final_Dataset[['Pct_Change_Month', 'Close_Price_t-5', 'Close_Price_t-10']] = \\\n",
    "    Final_Dataset[['Pct_Change_Month', 'Close_Price_t-5', 'Close_Price_t-10']].ffill()\n",
    "\n",
    "# Fill the last missing value in Bollinger_Upper with the column mean\n",
    "Final_Dataset['Bollinger_Upper'] = Final_Dataset['Bollinger_Upper'].fillna(Final_Dataset['Bollinger_Upper'].mean())\n",
    "\n",
    "# Verify again\n",
    "print(\"Final missing values:\\n\", Final_Dataset.isna().sum())\n",
    "\n",
    "\n",
    "# Verify if there are any remaining NaNs\n",
    "missing_values_after = Final_Dataset.isna().sum()\n",
    "print(\"Missing values after second imputation:\\n\", missing_values_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå STEP 1.7: Handling Class Imbalance using Class Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1Ô∏è‚É£ Data Preprocessing & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Extract features and target variable\n",
    "X_train = train.drop(columns=['Trend_Label'])  # Drop target column\n",
    "y_train = train['Trend_Label'].astype(int)\n",
    "\n",
    "X_test = test.drop(columns=['Trend_Label'])\n",
    "y_test = test['Trend_Label'].astype(int)\n",
    "\n",
    "# Drop non-numeric columns (e.g., Date, Ticker, or Company)\n",
    "X_train = X_train.select_dtypes(include=[np.number])\n",
    "X_test = X_test.select_dtypes(include=[np.number])\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_train_resampled = scaler.fit_transform(X_resampled)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_resampled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_resampled, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Move tensors to device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train_tensor, y_train_tensor = X_train_tensor.to(device), y_train_tensor.to(device)\n",
    "X_test_tensor, y_test_tensor = X_test_tensor.to(device), y_test_tensor.to(device)\n",
    "\n",
    "# Compute class weights for handling imbalance\n",
    "class_counts = torch.bincount(y_train_tensor)\n",
    "weights = 1.0 / (class_counts.float() + 1e-6)  # Prevent division by zero\n",
    "weights = weights / weights.sum()  # Normalize weights\n",
    "\n",
    "# Ensure correct shape (3 classes)\n",
    "if len(weights) != len(torch.unique(y_train_tensor)):\n",
    "    weights = torch.ones(len(torch.unique(y_train_tensor))).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "\n",
    "# Reshape tensors for LSTM (batch_size, time_steps, input_size)\n",
    "time_steps = 1  # Assuming a single time-step for input features\n",
    "X_train_tensor = X_train_tensor.view(X_train_tensor.shape[0], time_steps, -1)\n",
    "X_test_tensor = X_test_tensor.view(X_test_tensor.shape[0], time_steps, -1)\n",
    "\n",
    "# DataLoader for batch processing\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2Ô∏è‚É£ Define the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move data to the correct device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # Using the last output of the LSTM\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 10  # Example input size (adjust based on your dataset)\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "learning_rate = 0.0005\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize model, criterion, optimizer\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Learning Rate Scheduler (Optional)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.7)\n",
    "\n",
    "# Compute class weights for handling imbalance (ensure this has 3 values)\n",
    "class_counts = torch.bincount(y_train_tensor)\n",
    "weights = 1.0 / (class_counts.float() + 1e-6)  # Prevent division by zero\n",
    "\n",
    "# Ensure weights tensor has 3 values for 3 classes\n",
    "if len(weights) != 3:\n",
    "    weights = torch.ones(3).to(device)  # If not, use equal weights for 3 classes\n",
    "\n",
    "# Define criterion with updated weights\n",
    "criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "\n",
    "# Apply SMOTE for balancing classes\n",
    "X_train_resampled, y_train_resampled = SMOTE(random_state=42).fit_resample(X_train_tensor.view(X_train_tensor.shape[0], -1), y_train_tensor)\n",
    "\n",
    "# Convert back to tensors\n",
    "X_train_resampled = torch.tensor(X_train_resampled, dtype=torch.float32).view(X_train_tensor.shape)\n",
    "y_train_resampled = torch.tensor(y_train_resampled, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for resampled data\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train_resampled, y_train_resampled), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3Ô∏è‚É£ Data Preparation & Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.9030\n",
      "Epoch 2/10, Loss: 0.8206\n",
      "Epoch 3/10, Loss: 0.7786\n",
      "Epoch 4/10, Loss: 0.7226\n",
      "Epoch 5/10, Loss: 0.6745\n",
      "Epoch 6/10, Loss: 0.6283\n",
      "Epoch 7/10, Loss: 0.5978\n",
      "Epoch 8/10, Loss: 0.5616\n",
      "Epoch 9/10, Loss: 0.5209\n",
      "Epoch 10/10, Loss: 0.4814\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90     27483\n",
      "           1       0.92      0.79      0.85     15147\n",
      "           2       0.26      0.70      0.38      3058\n",
      "\n",
      "    accuracy                           0.82     45688\n",
      "   macro avg       0.71      0.78      0.71     45688\n",
      "weighted avg       0.90      0.82      0.85     45688\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAHHCAYAAACcHAM1AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUTtJREFUeJzt3XdYFFfbBvB7l7L0JtIsgGJDDdYgYuOViDWWGGsUjV0wKlaiUazE3pWYRDGWRI3RJGpUrFiwodgixoJBoxRFQDqy8/3Bx8YNqOAw7Ir3L9deV5g5c+acFfHhec6ZlQmCIICIiIhIi8k1PQAiIiKiN2HAQkRERFqPAQsRERFpPQYsREREpPUYsBAREZHWY8BCREREWo8BCxEREWk9BixERESk9RiwEBERkdZjwEIkodu3b6Ndu3YwNzeHTCbDnj17SrX/+/fvQyaTITQ0tFT7fZe1adMGbdq00fQwiKiUMWChcu/u3bsYMWIEqlWrBgMDA5iZmcHT0xMrVqxAZmampPf29fXFtWvXMG/ePGzevBlNmjSR9H5ladCgQZDJZDAzMyvyfbx9+zZkMhlkMhkWL15c4v4fPXqEoKAgREVFlcJoiehdp6vpARBJad++ffj000+hUCgwcOBA1KtXDzk5OTh16hQmTZqEGzduYP369ZLcOzMzExEREZg2bRr8/f0luYejoyMyMzOhp6cnSf9voquri4yMDPz+++/o1auX2rmtW7fCwMAAWVlZb9X3o0ePMGvWLDg5OaFBgwbFvu7QoUNvdT8i0m4MWKjciomJQZ8+feDo6IijR4/C3t5edc7Pzw937tzBvn37JLt/YmIiAMDCwkKye8hkMhgYGEjW/5soFAp4enrixx9/LBSwbNu2DZ06dcKuXbvKZCwZGRkwMjKCvr5+mdyPiMoWS0JUbi1cuBBpaWn4/vvv1YKVAi4uLhg7dqzq6xcvXmDOnDmoXr06FAoFnJyc8OWXXyI7O1vtOicnJ3Tu3BmnTp3Chx9+CAMDA1SrVg0//PCDqk1QUBAcHR0BAJMmTYJMJoOTkxOA/FJKwf+/LCgoCDKZTO1YWFgYWrRoAQsLC5iYmKBWrVr48ssvVedftYbl6NGjaNmyJYyNjWFhYYGuXbvi5s2bRd7vzp07GDRoECwsLGBubo7BgwcjIyPj1W/sf/Tr1w9//PEHkpOTVccuXLiA27dvo1+/foXaJyUlYeLEiahfvz5MTExgZmaGDh064MqVK6o2x48fR9OmTQEAgwcPVpWWCubZpk0b1KtXD5GRkWjVqhWMjIxU78t/17D4+vrCwMCg0Px9fHxgaWmJR48eFXuuRKQ5DFio3Pr9999RrVo1NG/evFjthw4dihkzZqBRo0ZYtmwZWrdujeDgYPTp06dQ2zt37qBnz5746KOPsGTJElhaWmLQoEG4ceMGAKBHjx5YtmwZAKBv377YvHkzli9fXqLx37hxA507d0Z2djZmz56NJUuW4OOPP8bp06dfe93hw4fh4+ODhIQEBAUFISAgAGfOnIGnpyfu379fqH2vXr3w/PlzBAcHo1evXggNDcWsWbOKPc4ePXpAJpPhl19+UR3btm0bateujUaNGhVqf+/ePezZswedO3fG0qVLMWnSJFy7dg2tW7dWBQ916tTB7NmzAQDDhw/H5s2bsXnzZrRq1UrVz9OnT9GhQwc0aNAAy5cvh5eXV5HjW7FiBSpWrAhfX1/k5eUBAL755hscOnQIq1atgoODQ7HnSkQaJBCVQykpKQIAoWvXrsVqHxUVJQAQhg4dqnZ84sSJAgDh6NGjqmOOjo4CACE8PFx1LCEhQVAoFMKECRNUx2JiYgQAwqJFi9T69PX1FRwdHQuNYebMmcLLfyWXLVsmABASExNfOe6Ce2zcuFF1rEGDBoKNjY3w9OlT1bErV64IcrlcGDhwYKH7ff7552p9du/eXahQocIr7/nyPIyNjQVBEISePXsKbdu2FQRBEPLy8gQ7Ozth1qxZRb4HWVlZQl5eXqF5KBQKYfbs2apjFy5cKDS3Aq1btxYACCEhIUWea926tdqxgwcPCgCEuXPnCvfu3RNMTEyEbt26vXGORKQ9mGGhcik1NRUAYGpqWqz2+/fvBwAEBASoHZ8wYQIAFFrr4urqipYtW6q+rlixImrVqoV79+699Zj/q2Dty6+//gqlUlmsax4/foyoqCgMGjQIVlZWquMffPABPvroI9U8XzZy5Ei1r1u2bImnT5+q3sPi6NevH44fP464uDgcPXoUcXFxRZaDgPx1L3J5/o+evLw8PH36VFXuunTpUrHvqVAoMHjw4GK1bdeuHUaMGIHZs2ejR48eMDAwwDfffFPsexGR5jFgoXLJzMwMAPD8+fNitf/7778hl8vh4uKidtzOzg4WFhb4+++/1Y5XrVq1UB+WlpZ49uzZW464sN69e8PT0xNDhw6Fra0t+vTpgx07drw2eCkYZ61atQqdq1OnDp48eYL09HS14/+di6WlJQCUaC4dO3aEqakptm/fjq1bt6Jp06aF3ssCSqUSy5YtQ40aNaBQKGBtbY2KFSvi6tWrSElJKfY9K1WqVKIFtosXL4aVlRWioqKwcuVK2NjYFPtaItI8BixULpmZmcHBwQHXr18v0XX/XfT6Kjo6OkUeFwThre9RsL6igKGhIcLDw3H48GEMGDAAV69eRe/evfHRRx8VaiuGmLkUUCgU6NGjBzZt2oTdu3e/MrsCAPPnz0dAQABatWqFLVu24ODBgwgLC0PdunWLnUkC8t+fkrh8+TISEhIAANeuXSvRtUSkeQxYqNzq3Lkz7t69i4iIiDe2dXR0hFKpxO3bt9WOx8fHIzk5WbXjpzRYWlqq7agp8N8sDgDI5XK0bdsWS5cuxZ9//ol58+bh6NGjOHbsWJF9F4zz1q1bhc5FR0fD2toaxsbG4ibwCv369cPly5fx/PnzIhcqF/j555/h5eWF77//Hn369EG7du3g7e1d6D0pbvBYHOnp6Rg8eDBcXV0xfPhwLFy4EBcuXCi1/olIegxYqNyaPHkyjI2NMXToUMTHxxc6f/fuXaxYsQJAfkkDQKGdPEuXLgUAdOrUqdTGVb16daSkpODq1auqY48fP8bu3bvV2iUlJRW6tuABav/dal3A3t4eDRo0wKZNm9QCgOvXr+PQoUOqeUrBy8sLc+bMwerVq2FnZ/fKdjo6OoWyNzt37sQ///yjdqwgsCoquCupKVOmIDY2Fps2bcLSpUvh5OQEX1/fV76PRKR9+OA4KreqV6+Obdu2oXfv3qhTp47ak27PnDmDnTt3YtCgQQAANzc3+Pr6Yv369UhOTkbr1q1x/vx5bNq0Cd26dXvlltm30adPH0yZMgXdu3fHF198gYyMDKxbtw41a9ZUW3Q6e/ZshIeHo1OnTnB0dERCQgLWrl2LypUro0WLFq/sf9GiRejQoQM8PDwwZMgQZGZmYtWqVTA3N0dQUFCpzeO/5HI5pk+f/sZ2nTt3xuzZszF48GA0b94c165dw9atW1GtWjW1dtWrV4eFhQVCQkJgamoKY2NjuLu7w9nZuUTjOnr0KNauXYuZM2eqtllv3LgRbdq0wVdffYWFCxeWqD8i0hAN71Iiktxff/0lDBs2THBychL09fUFU1NTwdPTU1i1apWQlZWlapebmyvMmjVLcHZ2FvT09IQqVaoIgYGBam0EIX9bc6dOnQrd57/baV+1rVkQBOHQoUNCvXr1BH19faFWrVrCli1bCm1rPnLkiNC1a1fBwcFB0NfXFxwcHIS+ffsKf/31V6F7/Hfr7+HDhwVPT0/B0NBQMDMzE7p06SL8+eefam0K7vffbdMbN24UAAgxMTGvfE8FQX1b86u8alvzhAkTBHt7e8HQ0FDw9PQUIiIiityO/Ouvvwqurq6Crq6u2jxbt24t1K1bt8h7vtxPamqq4OjoKDRq1EjIzc1Vazd+/HhBLpcLERERr50DEWkHmSCUYGUdERERkQZwDQsRERFpPQYsREREpPUYsBAREZHWY8BCREREWo8BCxEREWk9BixERESk9RiwEBERkdYrl0+6NWzor+khkJZJOr9a00MgLfIoOVPTQyAtUr1iyT5I822U1r9LmZff359lzLAQERGR1iuXGRYiIiKtImN+QCwGLERERFKTyTQ9gnceAxYiIiKpMcMiGt9BIiIi0nrMsBAREUmNJSHRGLAQERFJjSUh0fgOEhERkdZjhoWIiEhqLAmJxoCFiIhIaiwJicZ3kIiIiLQeMyxERERSY0lINAYsREREUmNJSDS+g0RERKT1mGEhIiKSGktCojFgISIikhpLQqIxYCEiIpIaMyyiMeQjIiIirccMCxERkdRYEhKNAQsREZHUGLCIxneQiIiItB4zLERERFKTc9GtWAxYiIiIpMaSkGh8B4mIiEjrMcNCREQkNT6HRTQGLERERFJjSUg0voNERESk9ZhhISIikhpLQqIxYCEiIpIaS0KiMWAhIiKSGjMsojHkIyIiIq3HDAsREZHUWBISjQELERGR1FgSEo0hHxEREWk9ZliIiIikxpKQaAxYiIiIpMaSkGgM+YiIiEjrMcNCREQkNZaERGPAQkREJDUGLKLxHSQiIiKtxwwLERGR1LjoVjQGLERERFJjSUg0BixERERSY4ZFNIZ8REREpPWYYSEiIpIaS0KiMWAhIiKSGktCojHkIyIiIq3HDAsREZHEZMywiMaAhYiISGIMWMRjSYiIiIi0HgMWIiIiqclK6VUCwcHBaNq0KUxNTWFjY4Nu3brh1q1bam2ysrLg5+eHChUqwMTEBJ988gni4+PV2sTGxqJTp04wMjKCjY0NJk2ahBcvXqi1OX78OBo1agSFQgEXFxeEhoYWGs+aNWvg5OQEAwMDuLu74/z58yWaDwMWIiIiiclkslJ5lcSJEyfg5+eHs2fPIiwsDLm5uWjXrh3S09NVbcaPH4/ff/8dO3fuxIkTJ/Do0SP06NFDdT4vLw+dOnVCTk4Ozpw5g02bNiE0NBQzZsxQtYmJiUGnTp3g5eWFqKgojBs3DkOHDsXBgwdVbbZv346AgADMnDkTly5dgpubG3x8fJCQkFD891AQBKFE78A7wLChv6aHQFom6fxqTQ+BtMij5ExND4G0SPWKhpLfw6RXaKn0k7Zj0Ftfm5iYCBsbG5w4cQKtWrVCSkoKKlasiG3btqFnz54AgOjoaNSpUwcRERFo1qwZ/vjjD3Tu3BmPHj2Cra0tACAkJARTpkxBYmIi9PX1MWXKFOzbtw/Xr19X3atPnz5ITk7GgQMHAADu7u5o2rQpVq/O/1msVCpRpUoVjBkzBlOnTi3W+JlhISIikpgmMiz/lZKSAgCwsrICAERGRiI3Nxfe3t6qNrVr10bVqlUREREBAIiIiED9+vVVwQoA+Pj4IDU1FTdu3FC1ebmPgjYFfeTk5CAyMlKtjVwuh7e3t6pNcXCXEBERkcRKa5dQdnY2srOz1Y4pFAooFIrXXqdUKjFu3Dh4enqiXr16AIC4uDjo6+vDwsJCra2trS3i4uJUbV4OVgrOF5x7XZvU1FRkZmbi2bNnyMvLK7JNdHR0MWadjxkWLTHx83Y4tWUSEk4txt9HgrFj6TDUcLRRa7NqWh/c+G0mkiKWIvZoMHYsG46aTrZF9mdlbow7B+Yg8/JqmJuopzv19XQR5NcFt/bPRvK5ZYjeNwsDuzZTna9TzQ4/Lh6K6H2zkHl5Nfz7tSn1+VLpWLdmFRrUq6X26talfaF2giDAb+RQNKhXC0ePHFY79/jxI/iPGo5mTdzg1coDSxcvKLSgjrTPvt07MNr3U3zSzhOftPNEwIiBuBBxqlA7QRDw1QQ/dGzRAGfCj6qd++vmdQSOHY5P27dAr/YtMT1gFO7d/ndR5tVLFzB76jj07+qN7t7N4D+oF44d2if53Mqj0sqwBAcHw9zcXO0VHBz8xvv7+fnh+vXr+Omnn8pgttJghkVLtGzkgpDt4Yi88Td0dXUwy78L9q7zR8Mec5GRlQMAuHzzAX764wIePH4GK3MjTBvZCXvX+qF255lQKtWXIoXM7Idrtx+hkq1loXttWfg5bK1MMXLWVtyNTYR9RXPIX4r+jQz0EfPwCX4Ju4wFE3oUup60S3WXGvjmu42qr3V0dAq12bJ5U5GPBs/Ly8OY0SNQoYI1Qrf8hCeJCfjqyynQ1dXDF+MCJB03iWNd0RaDR34Bh8pVIQjAkT9+w5zAcVi14Sc4VnNRtduzY0uRT4XPzMjAVxP84N6iNfwmfIm8Fy+wZUMIvpowGpt+OQBdXT3cvH4FTtVroGf/QbC0qoBzp8OxZO5XMDI2hbtnqzKcLRUIDAxEQID63803ZVf8/f2xd+9ehIeHo3LlyqrjdnZ2yMnJQXJyslqWJT4+HnZ2dqo2/93NU7CL6OU2/91ZFB8fDzMzMxgaGkJHRwc6OjpFtinooziYYdESXf3XYsvv53DzXhyu/fUPhs/cgqr2VmjoWkXVZsMvp3H60l3EPk5CVPRDzFrzO6rYW8HRoYJaX8M+bQFzUyMs/+FIoft81LwOWjZ2Qbcx63Ds3C3EPk7CuasxiLhyT9Um8s9YfLl8D3YejEROLn/T1nY6Ojqwtq6oellaWqmdj46+ic2bNmDWnPmFro04cwr37t7B/K8XoXbtOmjRsjVG+4/Fjp+2Ijc3p6ymQG/BvUVrNPVoiUpVHFG5qiN8R4yBgaERov+8pmpz93Y0fvlpM8YFzip0/YPYGDxPTcGAIaNRuaoTHKu5oN/gEXiW9BQJcY8BAL0HDsXAYX5wrd8A9pWqoFuv/mjs3hxnThT+2UJvUErbmhUKBczMzNRerwpYBEGAv78/du/ejaNHj8LZ2VntfOPGjaGnp4cjR/7987x16xZiY2Ph4eEBAPDw8MC1a9fUdvOEhYXBzMwMrq6uqjYv91HQpqAPfX19NG7cWK2NUqnEkSNHVG2KQ6MBy5MnT7Bw4UJ0794dHh4e8PDwQPfu3bFo0SIkJiZqcmgaZ2ZiAAB4lpJR5HkjA30M/LgZYh4+wcO4Z6rjtavZIXBYBwz96odCWRcA6NS6Pi79GYuAQd64e3Auru6ZgeDx3WGg0JNmIiS52Ni/8ZFXC3Rq3xaBUybg8eNHqnOZmZn4cvIEBE6bAWvrioWuvXolCi41aqKCtbXqWHPPFkhLS8PdO3fKZPwkXl5eHk4cPoCsrEzUqfsBACArKxMLZ32J0QGBsKpgXeiaylWdYGZugYN7dyM3NxfZ2Vk4tHc3qjhVg62dwyvvlZ6WBlMzc8nmUl5pYtGtn58ftmzZgm3btsHU1BRxcXGIi4tDZmb+Ljlzc3MMGTIEAQEBOHbsGCIjIzF48GB4eHigWbP8ZQLt2rWDq6srBgwYgCtXruDgwYOYPn06/Pz8VIHSyJEjce/ePUyePBnR0dFYu3YtduzYgfHjx6vGEhAQgG+//RabNm3CzZs3MWrUKKSnp2Pw4MHFno/GSkIXLlyAj48PjIyM4O3tjZo1awLITxGtXLkSX3/9NQ4ePIgmTZpoaogaI5PJsGhiT5y5fBd/3n2sdm74py0xb1w3mBgpcCsmDp1GrUbuizwA+WtTNgUPwpfL9+BB3DM4VSr8Q8q5kjWaN6iOrOwX6B3wLSpYGmNFYG9YmRtjRNCWMpkflZ76H3yA2XOD4eTkjCdPEhGydg0+H9gfP+/5HcbGJli8MBhuDRrC63/eRV7/5MkTVPjPP2YF/7g9efJ+/9LwLoi5exsTRg5ETk4ODA0N8dX8pajqXB0A8O3KxahTzw0eLb2KvNbIyBhfr/oOcwLH46dN3wIAHCpXxZyla6GjW/Q/DeFHDuKv6BsYM2m6NBOiUrVu3ToAQJs2bdSOb9y4EYMGDQIALFu2DHK5HJ988gmys7Ph4+ODtWvXqtrq6Ohg7969GDVqFDw8PGBsbAxfX1/Mnj1b1cbZ2Rn79u3D+PHjsWLFClSuXBnfffcdfHx8VG169+6NxMREzJgxA3FxcWjQoAEOHDhQaCHu62gsYBkzZgw+/fRThISEFIoaBUHAyJEjMWbMmDdueSpqxbSgzINMXriO/65YHtgLdV3s0XbwskLnfvrjAo6ci4adtRnGDfTGlgWf43+DlyI75wXmfPExbsXE46f9F17Zt1wugyAIGDwtFKlpWQCAKUt+wbZFQzA2eDuysnMlmxeVvhYtW6v+v2at2qhX3w0d23nh0IE/YGllhfPnzmL7z7s1OEKSUuWqTli9cTvS09Jw6vhhLJk3AwtXfYdH/zzAlUvnsWrD9ldem52dheXBQXCt74YpQcFQ5imx66cfEDRpDJZ/txUKhYFa+yuXLmBZ8EyMnTxDbY0MFY8mPkuoOI9ZMzAwwJo1a7BmzZpXtnF0dMT+/ftf20+bNm1w+fLl17bx9/eHv//bPydNYwHLlStXEBoaWuQfokwmw/jx49GwYcM39hMcHIxZs9Trszq2TaFn/2GpjbUsLZvyKTq2rAfvIcvxT0JyofOpaVlITcvC3dhEnL96H4/DF6Lr/9yw40AkWjetiXouDuh+oQGAf/+CPDz2NRZ8fxBzQ/Yj7kkqHiWkqIIVAIiOiYNcLkclWwvcjeVv1e8yMzMzVHV0woPYWNy+/RcePohFS4+mam0mjh+Dho2a4PvQzbC2tsb1a1fVzic9fQIARZaQSLvo6enBoXJVAECN2q64ffMGft25DfoKBR7/8xCfdmip1n7+9Imo+0FDLFj9PY6H/YGEuEdY+s0PkMvzVwdMnhmMXh1a4uzJ42jt/e9us2uXL2LWlC8wfMxEtO3QpczmV57www/F01jAUrDyuHbt2kWeP3/+fLFSRUWtmLZpOaVUxljWlk35FB//zw3thq3A34+evrG9TCaDDDLo6+X/Mfad+B0MX1qL0riuI9bP+gzeQ5bj3oP8QCQi6h56eDeEsaE+0jPzF1XWcLRBXp4S/8Qnl/6kqExlZKTj4YMHsO5SEe3ad0CPTz5VO9+zexdMnByI1m3yywQfuDXAd+tDkPT0Kawq5C/ejog4AxMTE1Srzt+i3zVKQYnc3Bz0HzIKPl3Ud/iNHtgTw8ZMhLtnflYuOysLMrlc7R9S+f+vk1AqlapjVy9dQNCULzB45Fh06NqzbCZCVASNBSwTJ07E8OHDERkZibZt26qCk/j4eBw5cgTffvstFi9e/MZ+inpgzrtYDloe2Au9OzTBp+PXIy09C7YVTAEAKWlZyMrOhVOlCujp0xhHIm7iybM0VLK1wITB7ZCZnYuDp/KfNhjz8IlanxUsTAAA0ffikJKWv8hq+x8XEDisPdbP+gxzQvajgoUx5o/rjk2/RqjKQXq6OqhTLX+rmb6eLhxsLPBBzUpIy8zGvQfq9yDNWrpoAVq18YK9gwMSExKwbs0q6OjI0b5jZ1hZWRWZJbGzd0Clyvm7zzyat0C16i6YFjgZ4wIm4enTRKxZtRy9+vSHvr5+WU+HSmBjyEo0aeYJG1s7ZGRk4HjYH7h2+SLmLF0LqwrWRS60rWhrBzuHSgCAhk2b4fu1y7B2yXx06dkXglKJHVs3QkdHB26N8rNyVy5dQNDkMej6aT94tvFWZd/09PS48LaEmGERT2MBi5+fH6ytrbFs2TKsXbsWeXn5C0d1dHTQuHFjhIaGolevXpoaXpkb0Sv/mQZh341TOz5sxmZs+f0csnNewLNhdfj3awNLMyMkPH2OU5fuwGvQEiQ+Syv2fdIzc9Bp1GosnfIpTm+ZjKSUdOwKu4SgNXtVbewrmuPc9kDV1+N9vTHe1xvhF2/DZ9gKcROlUhUfH4fAyQFITk6GpZUVGjZsjB+27lA9evtNdHR0sHJNCObNCYLvZ71haGiILh93x2j/LyQeOYmV8iwJS+ZOR9LTJzA2NoFz9ZqYs3QtGjUt3jbRKo7OmLlgBbZt+AYTRg6ETCZH9Zq1MWfxWlj9f6B75I/fkJ2VhR2bN2DH5g2qa+s3aIwFq7+XZF7lFuMV0bTiww9zc3Px5ElB3dwaenrittjyww/pv/jhh/QyfvghvawsPvywgu+PpdLP0019S6Wfd5FWPOlWT08P9vb2mh4GERGRJFgSEk8rAhYiIqLyjAGLeAxYiIiIJMaARTx+lhARERFpPWZYiIiIpMYEi2gMWIiIiCTGkpB4LAkRERGR1mOGhYiISGLMsIjHgIWIiEhiDFjEY0mIiIiItB4zLERERBJjhkU8BixERERSY7wiGktCREREpPWYYSEiIpIYS0LiMWAhIiKSGAMW8RiwEBERSYwBi3hcw0JERERajxkWIiIiqTHBIhoDFiIiIomxJCQeS0JERESk9ZhhISIikhgzLOIxYCEiIpIYAxbxWBIiIiIirccMCxERkcSYYRGPAQsREZHUGK+IxpIQERERaT1mWIiIiCTGkpB4DFiIiIgkxoBFPAYsREREEmO8Ih7XsBAREZHWY4aFiIhIYiwJiceAhYiISGKMV8RjSYiIiIi0HjMsREREEmNJSDwGLERERBJjvCIeS0JERESk9ZhhISIikphczhSLWAxYiIiIJMaSkHgsCREREZHWY4aFiIhIYtwlJB4DFiIiIokxXhGPAQsREZHEmGERj2tYiIiISOsxw0JERCQxZljEY8BCREQkMcYr4rEkRERERFqPGRYiIiKJsSQkHgMWIiIiiTFeEY8lISIiItJ6zLAQERFJjCUh8RiwEBERSYzxingsCREREZHWY4aFiIhIYiwJiceAhYiISGKMV8RjwEJERCQxZljE4xoWIiIi0nrlMsPy9PwqTQ+BtMzEvTc1PQTSItO8qmt6CPSeYYJFvHIZsBAREWkTloTEY0mIiIiItB4zLERERBJjgkU8BixEREQSY0lIPJaEiIiISOsxw0JERCQxJljEY8BCREQkMZaExGNJiIiIqJwKDw9Hly5d4ODgAJlMhj179qidHzRoEGQymdqrffv2am2SkpLQv39/mJmZwcLCAkOGDEFaWppam6tXr6Jly5YwMDBAlSpVsHDhwkJj2blzJ2rXrg0DAwPUr18f+/fvL9FcGLAQERFJ7L9Bwdu+Sio9PR1ubm5Ys2bNK9u0b98ejx8/Vr1+/PFHtfP9+/fHjRs3EBYWhr179yI8PBzDhw9XnU9NTUW7du3g6OiIyMhILFq0CEFBQVi/fr2qzZkzZ9C3b18MGTIEly9fRrdu3dCtWzdcv3692HNhSYiIiEhimqoIdejQAR06dHhtG4VCATs7uyLP3bx5EwcOHMCFCxfQpEkTAMCqVavQsWNHLF68GA4ODti6dStycnKwYcMG6Ovro27duoiKisLSpUtVgc2KFSvQvn17TJo0CQAwZ84chIWFYfXq1QgJCSnWXJhhISIikpimMizFcfz4cdjY2KBWrVoYNWoUnj59qjoXEREBCwsLVbACAN7e3pDL5Th37pyqTatWraCvr69q4+Pjg1u3buHZs2eqNt7e3mr39fHxQURERLHHyQwLERHROyI7OxvZ2dlqxxQKBRQKxVv11759e/To0QPOzs64e/cuvvzyS3To0AERERHQ0dFBXFwcbGxs1K7R1dWFlZUV4uLiAABxcXFwdnZWa2Nra6s6Z2lpibi4ONWxl9sU9FEczLAQERFJTCYrnVdwcDDMzc3VXsHBwW89rj59+uDjjz9G/fr10a1bN+zduxcXLlzA8ePHS2/ypYQZFiIiIomVVjknMDAQAQEBasfeNrtSlGrVqsHa2hp37txB27ZtYWdnh4SEBLU2L168QFJSkmrdi52dHeLj49XaFHz9pjavWjtTFGZYiIiI3hEKhQJmZmZqr9IMWB4+fIinT5/C3t4eAODh4YHk5GRERkaq2hw9ehRKpRLu7u6qNuHh4cjNzVW1CQsLQ61atWBpaalqc+TIEbV7hYWFwcPDo9hjY8BCREQksdIqCZVUWloaoqKiEBUVBQCIiYlBVFQUYmNjkZaWhkmTJuHs2bO4f/8+jhw5gq5du8LFxQU+Pj4AgDp16qB9+/YYNmwYzp8/j9OnT8Pf3x99+vSBg4MDAKBfv37Q19fHkCFDcOPGDWzfvh0rVqxQywSNHTsWBw4cwJIlSxAdHY2goCBcvHgR/v7+xZ4LAxYiIiKJyWWyUnmV1MWLF9GwYUM0bNgQABAQEICGDRtixowZ0NHRwdWrV/Hxxx+jZs2aGDJkCBo3boyTJ0+qZW22bt2K2rVro23btujYsSNatGih9owVc3NzHDp0CDExMWjcuDEmTJiAGTNmqD2rpXnz5ti2bRvWr18PNzc3/Pzzz9izZw/q1atX7LnIBEEQSvwOaLmM3HI3JRJp0t5oTQ+BtMg0r+qaHgJpEQcL/Tc3Eumj1WdLpZ8w/2al0s+7iItuiYiIJMaPEhKPAQsREZHE+OGH4jFgISIikpic8YpoXHRLREREWo8ZFiIiIomxJCQeAxYiIiKJMV4RjyUhIiIi0nrMsBAREUlMBqZYxGLAQkREJDHuEhKPJSEiIiLSesywEBERSYy7hMRjwEJERCQxxivisSREREREWo8ZFiIiIonJmWIRjQELERGRxBiviMeAhYiISGJcdCse17AQERGR1mOGhYiISGJMsIjHgIWIiEhiXHQrHktCREREpPWYYSEiIpIY8yviMWAhIiKSGHcJiceSEBEREWk9ZliIiIgkJmeCRTQGLERERBJjSUg8loSIiIhI6zHDQkREJDEmWMRjwEJERCQxloTEY8BCREQkMS66FY9rWIiIiEjrvVXAcvLkSXz22Wfw8PDAP//8AwDYvHkzTp06VaqDIyIiKg9kMlmpvN5nJQ5Ydu3aBR8fHxgaGuLy5cvIzs4GAKSkpGD+/PmlPkAiIqJ3nayUXu+zEgcsc+fORUhICL799lvo6empjnt6euLSpUulOjgiIiIi4C0W3d66dQutWrUqdNzc3BzJycmlMSYiIqJyRf6el3NKQ4kzLHZ2drhz506h46dOnUK1atVKZVBERETliUxWOq/3WYkDlmHDhmHs2LE4d+4cZDIZHj16hK1bt2LixIkYNWqUFGMkIiKi91yJS0JTp06FUqlE27ZtkZGRgVatWkGhUGDixIkYM2aMFGMkIiJ6p73vO3xKQ4kDFplMhmnTpmHSpEm4c+cO0tLS4OrqChMTEynGR/+REB+PFUsX4/SpcGRlZaFK1aoImjMfdevVL9R27qyZ2LVzOyZOCUT/Ab6q4zf/vIEVS5fgxo1r0JHL0fajdpgweSqMjIzLcir0Bi4VDOFdowKqWBjAwlAP35x9gKuP0wDkP4Sqi2tF1LU1gbWxPjJz83ArMR2/3khEStYLVR9GenL0crNDPTsTCAIQ9eg5fr4ah+w8QdXGwUyB3m52cLQ0QFp2Ho7fS8Lh20mq882dLOBexRwOZgoAQGxyFn77MwF/P8sqo3eCirI19DucPH4YsX/HQKEwQN36bhjuPx5VHZ0LtRUEAVPHj8L5iNOYs3A5WrRuqzoXeeEsNn6zGvfu3oaBgSF8On2MoSO/gI6urtr1O7Zuwt49PyM+7hHMLSzR9ZPe+Gzw8DKZa3nAeEW8t37Srb6+PlxdXUtzLPQGqSkpGDSgL5p+6I7VId/C0tIKsX/fh5mZeaG2Rw+H4drVK6hoY6N2PCEhHiOHfo527Ttg6rTpSE9Lx6IF8zFjWiAWL1tZVlOhYtDXleNhSjYi/k7B8GaV1c/pyFHFwgAHbj3Bw5RsGOnJ8ekHdhjRrDIWHr+vajeoSSWYG+hi9elY6Mhl+KyRA/o2tEfoxUcAAANdOcZ4VkV0Qjp+inoMBzMDfNbIHpm5Spy+nwwAqGlthIsPUxGTlIHcPAEf1awA/+ZVMffIPbXgiMrWlcsX0a1nH9RyrYe8F3n4bt0KTP5iBDb+tAeGhkZqbX/+aTNkRWyKvfPXLQSOH43+g4YhcOZ8JCbGY9mCOVDmKTFq7ERVu1VLv8bFc2cw8osJqOZSA6kpKXiemiL5HIleVuKAxcvL67WpraNHj4oaEL3axg3fwc7OHrPmBquOVapcuVC7hPh4LAiei7XffIcxo0eonTt54jh0dXUROH0G5PL8JUzTZgShV4+uiI39G1WrOko7CSq2P+PT8Wd8epHnsl4osfr0A7Vj26/EYYqXMywNdfEs8wVsTfVR184EC47FIDY5Pxuy80ocRjWvgt3XE5CS9QJNq5hBRy7DlkuPkCcAj5/noLKFAv9zsVIFLAXBTYGtlx6jQWdT1KpojPMP+I+WpixcEaL29dQZc9G9fWv8Ff0n3Bo2UR2/81c0dmzdhG82bccnHb3Urjl2+ACqudSE79D89YeVqlTFCP8AzJo2Eb5DR8HI2Bh/x9zDb7t2YMOPv6iyN/YOhX/u0Otxl5B4JV5026BBA7i5ualerq6uyMnJwaVLl1C/fuGyBJWeE8eOwrVuPUwKGIv/tWqOPj2745efd6i1USqVmB44Gb6DhqC6S41CfeTk5EBPT08VrACAwsAAABB1KVLaCZCkDPXkUAoCMnOVAIBqVobIyMlTBSsAEJ2YDkEAnCwNAQDOVoa48yQDL1WI8Gd8OuxMFTDUK/rHg76uHDpyGTJy86SbDJVYelp+ufDljGtWVibmfjUFYydNg1UF60LX5ObmQF9foXZMoVAgJzsbf0X/CQA4c+o4HCpVxtlT4ejbrT36dPPBonkzkZrCYLUkuEtIvBJnWJYtW1bk8aCgIKT9/18YksY/Dx9g5/Yf8dnAQRgybARuXL+GhcHzoKunh4+7dgcAbPz+W+jo6KDvZwOK7OND92ZYumgBNm34Hv0GDEBmRiZWLlsCAEhMTCyzuVDp0pXL0K2uDSIfpiLrRX7AYqbQxfNs9ZKNUgAycvNgZqCjavM0I1etTcE1Zga6yMzNKXSvbnVtkJL5AtEJRWd/qOwplUqsXrYA9T5oCOfq//6ismbZQtT9oAFatP5fkdc1dffErp+24MjB/Wjj7YOkp0/ww/f5mZunT/J/Hjz+5yHi4h7h+JFDCJw5D0qlEmuWL0RQYACWrv1e+smVE1x0K16pffjhZ599hg0bNpRWdwCABw8e4PPPP39tm+zsbKSmpqq9Cj4uoLxRKgXUruOKMeMCULuOKz75tDe6f/Ipft7xEwDgzxvX8eOWzZg1L/iVfzmqu9TA7HnB2LxpIzyaNIR3mxaoVKkyKlSwVsu60LtDLgOGfFgJkMnwU1ScpPf6qGYFNK5shvXnHuKFUnjzBVQmViyah5h7dzBj7kLVsdPhx3D54nn4j5/yyuuaNmuOEWMCsGzBHLRr2RgDP+0C9+YtAQCy//95oFQqkZuTg8CgefigYWM0aNwUk6bNwuXI84j9O0baiRG9pNT+hYqIiIDB/5cWSktSUhI2bdr02jbBwcEwNzdXey1eEPzaa95V1hUrolp1F7VjztWqI+7xYwDA5UuRSEp6io4f/Q9N3OqiiVtdPH70CEsXLUDHdv/+htWhUxccPnEKB4+cwPHTZzFytD+ePUtC5cpVynQ+JF5+sFIZVkZ6WH06VpVdAYDU7BcwVegWam+kp4PUrLxXtin4OvU/C2rbulihXY0KWH06Fo9Sy+cvBe+iFYvmIeLUCSxb+z0q2tqpjl++eB6P/nmAzt7N0bZ5A7Rt3gAAMHNqAMaNGqxq16ufL34/cgbbfz2EPQfD4dkq/2eFQ6X8dSoVrCtCR0cXVao6qa5xdMp/SGhC3GOJZ1d+yEvp9T4rcUmoR48eal8LgoDHjx/j4sWL+Oqrr0rU12+//fba8/fu3XtjH4GBgQgICFA7lifXL9E43hUNGjbE3/fVf6OJ/fs+7O0dAACdunwM92YeaudHjxiKTl26omu37oX6q2CdX9Pe88su6CsUaObRXKKRkxQKghUbEz2sOBmL9Bz1NSX3kjJhpK+DKhYGePD/61hqVjSGTAbcf5YJAIhJykQXVxvIZfnlIgCoY2OMuOfZqrUwAOBdwwrta1lj9elYtTUxpDmCIGDl4vk4deIolq3dUGghbD/fIejUVf3n9ef9emD0uMlo3rK12nGZTAbrivk7Co8c2g8bWzvUqFUHAFDvg4bIy3uBfx4+QKX//6XmQezfAADb///ZQ2/GkpB4JQ5YzM3Vt9DK5XLUqlULs2fPRrt27UrUV7du3SCTySAIr04tv+kPWaFQQKFQXzSWkVs+U9WfDRiEQQP64vv1IfiofQfcuHYVu37ega9mzgYAWFhYwsLCUu0aXV1dWFtbw8n5349N+GnbFrg1aAgjIyOcjTiD5UsWYcy4AJiamZXpfOj1FDoyVDT5N/iuYKSPyuYKpOfkISXrBYa5V0YVcwOsi3gAuQwwU+SvS0nPyUOeAMQ/z8GNuDT0a2iPn6IeQ0cmQy83W0Q+TFVtR77wIBUda1fEZ43sEfbXU9ibKdCmuhV2XYtX3fejGhXQqY41Qi8+QlJGruo+2S+Uas9zobK1fNE8HDm4H3MXrYCRsTGSnj4BABgbm0BhYACrCtZFLrS1tbNTC25+2rwRH3p4QiaX4+Sxw/jxh+8xc/5i6Ojk/zk3/rAZatSqg4Vzv4L/+ClQKpVYsWg+mnzooZZ1IZJaiQKWvLw8DB48GPXr14elpeWbL3gDe3t7rF27Fl27di3yfFRUFBo3biz6PuVF3fr1sWT5KqxasRTrQ9aiUqXKmDQlEB07dylRP9evXUPImlXIyMiAk3M1TJsxC50/LvrPgDSnqqUhxrX8d5t5zw9sAQBn/07Gvugn+MDeFADwZVv1z/BafvJv3H6SAQAIvfgPernZ4QvPqhCQ/+C4nVf+XeeS9UKJVadj0dvNDlO8nJGWk4c/op+otjQDQEtnC+jpyDHMXf03+H03E7E/+klpTplK4Ldd2wEA40epr/Ob8tUctO/crdj9nI84hS2h3yI3NwfVXWph7qKVqnUsQP4vpfOXrMbKxcEYO3IQDAwM4e7RAqPGTiqVebwv5EywiCYTXpfeKIKBgQFu3rwJZ+fCT1MsqY8//hgNGjTA7Nmzizx/5coVNGzYEEqlssjzr1JeMyz09ibtjdb0EEiLTPOqrukhkBZxsJB+GUHAb6XzM2jpx7VLpZ93UYlLQvXq1cO9e/dKJWCZNGkS0tNfvTXSxcUFx44dE30fIiIiereVOGCZO3cuJk6ciDlz5qBx48YwNlb//BmzEqyDaNmy5WvPGxsbo3Xr1q9tQ0REpO246Fa8Ygcss2fPxoQJE9CxY0cA+eWcl/8ABEGATCZDXh6ffklERPQyrmERr9gBy6xZszBy5EiWaIiIiKjMFTtgKVibyxINERFRybAiJF6J1rCwBkdERFRy/LRm8UoUsNSsWfONQUtSUpKoAREREZU37/tj9UtDiQKWWbNmFXrSLREREZHUShSw9OnTBzY2NlKNhYiIqFxiRUi8YgcsXL9CRET0driGRbxil9VK+AR/IiIiolJT7AxLST/Ph4iIiPIxwSJeiR/NT0RERCXDJ92Kx51WREREpPWYYSEiIpIYF92Kx4CFiIhIYoxXxGNJiIiIiLQeMyxEREQS46Jb8RiwEBERSUwGRixiMWAhIiKSGDMs4nENCxEREWk9BixEREQSk8tK51VS4eHh6NKlCxwcHCCTybBnzx6184IgYMaMGbC3t4ehoSG8vb1x+/ZttTZJSUno378/zMzMYGFhgSFDhiAtLU2tzdWrV9GyZUsYGBigSpUqWLhwYaGx7Ny5E7Vr14aBgQHq16+P/fv3l2guDFiIiIgkJpPJSuVVUunp6XBzc8OaNWuKPL9w4UKsXLkSISEhOHfuHIyNjeHj44OsrCxVm/79++PGjRsICwvD3r17ER4ejuHDh6vOp6amol27dnB0dERkZCQWLVqEoKAgrF+/XtXmzJkz6Nu3L4YMGYLLly+jW7du6NatG65fv17891Aoh59qmJFb7qZEIk3aG63pIZAWmeZVXdNDIC3iYKEv+T0WHb9XKv1MalPtra+VyWTYvXs3unXrBiA/u+Lg4IAJEyZg4sSJAICUlBTY2toiNDQUffr0wc2bN+Hq6ooLFy6gSZMmAIADBw6gY8eOePjwIRwcHLBu3TpMmzYNcXFx0NfPfy+nTp2KPXv2IDo6/2dv7969kZ6ejr1796rG06xZMzRo0AAhISHFGj8zLERERBLTVEnodWJiYhAXFwdvb2/VMXNzc7i7uyMiIgIAEBERAQsLC1WwAgDe3t6Qy+U4d+6cqk2rVq1UwQoA+Pj44NatW3j27Jmqzcv3KWhTcJ/i4C4hIiIiiZXWk26zs7ORnZ2tdkyhUEChUJS4r7i4OACAra2t2nFbW1vVubi4ONjY2Kid19XVhZWVlVobZ2fnQn0UnLO0tERcXNxr71MczLAQERG9I4KDg2Fubq72Cg4O1vSwygQzLERERBIrrQ8/DAwMREBAgNqxt8muAICdnR0AID4+Hvb29qrj8fHxaNCggapNQkKC2nUvXrxAUlKS6no7OzvEx8ertSn4+k1tCs4XBzMsREREEiutNSwKhQJmZmZqr7cNWJydnWFnZ4cjR46ojqWmpuLcuXPw8PAAAHh4eCA5ORmRkZGqNkePHoVSqYS7u7uqTXh4OHJzc1VtwsLCUKtWLVhaWqravHyfgjYF9ykOBixERETlVFpaGqKiohAVFQUgf6FtVFQUYmNjIZPJMG7cOMydOxe//fYbrl27hoEDB8LBwUG1k6hOnTpo3749hg0bhvPnz+P06dPw9/dHnz594ODgAADo168f9PX1MWTIENy4cQPbt2/HihUr1DJBY8eOxYEDB7BkyRJER0cjKCgIFy9ehL+/f7HnwpIQERGRxEpr0W1JXbx4EV5eXqqvC4IIX19fhIaGYvLkyUhPT8fw4cORnJyMFi1a4MCBAzAwMFBds3XrVvj7+6Nt27aQy+X45JNPsHLlStV5c3NzHDp0CH5+fmjcuDGsra0xY8YMtWe1NG/eHNu2bcP06dPx5ZdfokaNGtizZw/q1atX7LnwOSz0XuBzWOhlfA4LvawsnsOy5vT9UunHz9OpVPp5FzHDQkREJDFNZVjKE65hISIiIq3HDAsREZHESvspte8jBixEREQSK63nsLzPWBIiIiIirccMCxERkcSYYBGPAQsREZHEWBISjyUhIiIi0nrMsBAREUmMCRbxGLAQERFJjOUM8fgeEhERkdZjhoWIiEhiMtaERGPAQkREJDGGK+IxYCEiIpIYtzWLxzUsREREpPWYYSEiIpIY8yviMWAhIiKSGCtC4rEkRERERFqPGRYiIiKJcVuzeAxYiIiIJMZyhnh8D4mIiEjrMcNCREQkMZaExGPAQkREJDGGK+KxJERERERajxkWIiIiibEkJF65DFj4mQ30X4s719H0EEiLpGbmanoI9J5hOUO8chmwEBERaRNmWMRj0EdERERajxkWIiIiiTG/Ih4DFiIiIomxIiQeS0JERESk9ZhhISIikpicRSHRGLAQERFJjCUh8VgSIiIiIq3HDAsREZHEZCwJicaAhYiISGIsCYnHkhARERFpPWZYiIiIJMZdQuIxYCEiIpIYS0LiMWAhIiKSGAMW8biGhYiIiLQeMyxEREQS47Zm8RiwEBERSUzOeEU0loSIiIhI6zHDQkREJDGWhMRjwEJERCQx7hISjyUhIiIi0nrMsBAREUmMJSHxGLAQERFJjLuExGNJiIiIiLQeMyxEREQSY0lIPAYsREREEuMuIfEYsBAREUmM8Yp4XMNCREREWo8ZFiIiIonJWRMSjQELERGRxBiuiMeSEBEREWk9ZliIiIikxhSLaAxYiIiIJMbnsIjHkhARERFpPWZYiIiIJMZNQuIxYCEiIpIY4xXxWBIiIiIirccMCxERkdSYYhGNAQsREZHEuEtIPAYsREREEuOiW/G4hoWIiIi0HjMsREREEmOCRTwGLERERFJjxCIaS0JERESk9RiwEBERSUxWSv+VRFBQEGQymdqrdu3aqvNZWVnw8/NDhQoVYGJigk8++QTx8fFqfcTGxqJTp04wMjKCjY0NJk2ahBcvXqi1OX78OBo1agSFQgEXFxeEhoa+9fv0OgxYiIiIJCaTlc6rpOrWrYvHjx+rXqdOnVKdGz9+PH7//Xfs3LkTJ06cwKNHj9CjRw/V+by8PHTq1Ak5OTk4c+YMNm3ahNDQUMyYMUPVJiYmBp06dYKXlxeioqIwbtw4DB06FAcPHhT1fhVFJgiCUOq9aljWize3ofdL+fsuJzFSM3M1PQTSIrZmepLfIyr2ean006CqabHbBgUFYc+ePYiKiip0LiUlBRUrVsS2bdvQs2dPAEB0dDTq1KmDiIgINGvWDH/88Qc6d+6MR48ewdbWFgAQEhKCKVOmIDExEfr6+pgyZQr27duH69evq/ru06cPkpOTceDAAXGT/Q9mWIiIiCQmK6VXdnY2UlNT1V7Z2dmvvO/t27fh4OCAatWqoX///oiNjQUAREZGIjc3F97e3qq2tWvXRtWqVREREQEAiIiIQP369VXBCgD4+PggNTUVN27cULV5uY+CNgV9lCYGLERERFIrpYglODgY5ubmaq/g4OAib+nu7o7Q0FAcOHAA69atQ0xMDFq2bInnz58jLi4O+vr6sLCwULvG1tYWcXFxAIC4uDi1YKXgfMG517VJTU1FZmbmW7xRr8ZtzURERO+IwMBABAQEqB1TKBRFtu3QoYPq/z/44AO4u7vD0dERO3bsgKGhoaTjlAIzLERERBIrrV1CCoUCZmZmaq9XBSz/ZWFhgZo1a+LOnTuws7NDTk4OkpOT1drEx8fDzs4OAGBnZ1do11DB129qY2ZmVupBEQMWIiIiiWlql9DL0tLScPfuXdjb26Nx48bQ09PDkSNHVOdv3bqF2NhYeHh4AAA8PDxw7do1JCQkqNqEhYXBzMwMrq6uqjYv91HQpqCP0sSAhYiISGKltei2JCZOnIgTJ07g/v37OHPmDLp37w4dHR307dsX5ubmGDJkCAICAnDs2DFERkZi8ODB8PDwQLNmzQAA7dq1g6urKwYMGIArV67g4MGDmD59Ovz8/FRZnZEjR+LevXuYPHkyoqOjsXbtWuzYsQPjx48X94YVgWtYiIiIyqGHDx+ib9++ePr0KSpWrIgWLVrg7NmzqFixIgBg2bJlkMvl+OSTT5CdnQ0fHx+sXbtWdb2Ojg727t2LUaNGwcPDA8bGxvD19cXs2bNVbZydnbFv3z6MHz8eK1asQOXKlfHdd9/Bx8en1OfD57C8Q3b8tA07tv+IR//8AwCo7lIDI0aNRouWrQEAD2JjsWTxAkRdikROTg48W7TE1C+/QgVra1UfKcnJ+Hr+HJw4fgxyuRxtP2qHKVOnwcjYWCNzKivl77s837o1q/DNutVqx5ycnbHn9wNISUnGujWrEHHmFOIeP4alpRW8/ueN0WPGwtS08LMckpOfodcnXZEQH4/wMxdgZmZWVtMoc+XhOSxbNn6L8GOH8fffMVAoDFDvgwYY6T8eVZ2cVW1++2UnDh/ch79u3URGejr2HT0DU9Oi/1xzcnIwclBf3Ll9C99v+Rk1atUu1Obhg1gM+awndOQ62H+s9LetakpZPIfl+j9ppdJPvUompdLPu4gloXeIja0dxo6fiB93/oJtO3bhQ/dmGOvvhzt3biMjIwMjh38OmUyGbzdswqYtPyI3Nxdj/EZCqVSq+gicMhF379xByHcbsXJNCC5dvIjZQTNec1fSdtVdauDw8VOq18YftgEAEhMSkJiQgICJU/Dz7r2YPS8Yp0+fxKwZ04rsJ2jGNNSoWassh04iRF26iO6f9kXIhm1Yuno9XrzIxYQxw5GZmaFqk5WVhQ89WuCzQcPe2N+6lUtQoaLNK8+/eJGL2dMm4YMGjUtl/O8bTTyav7xhSegd0sbrf2pfjxk7Hjt++hFXr0QhIT4ej/75B9t/3gMTk/wIfM78BWjp0RTnz51FM4/muHf3Lk6fOolt239G3Xr1AQBTv5wOv1HDETBpMmxsbAvdk7Sfjo4OrK0rFjruUqMmlixfpfq6StWq8P9iHKZNzf8sEF3df//67/hpG56nPseIUaNx+mR4mYybxFm86hu1r7+cOQ8ft2uFWzf/RINGTQAAvfoNAABcjjz/2r7Onj6JC+fOYO6C5Th35mSRbb5dtwpVnZzRuGkz3LgaJX4CRCXEDMs7Ki8vD3/s34fMzAy4uTVETk4OZDIZ9PX1VW0UCgXkcjkuX4oEAFy5chmmZmaqYAUA3D2aQy6X49rVq2U+ByodsbF/4yOvFujUvi0Cp0zA48ePXtk27XkaTExM1IKVu3fvYH3IWswNXgCZjD8S3lVpafklBzMz8xJdl/T0CRbND8L0WcFQGBgU2SbywjkcP3wI4ydPFz3O95U27BJ61zHD8o65/dctDOjXBzk52TAyMsKylWtQ3cUFllZWMDQ0xPIlizBmXAAEQcCKZUuQl5eHxMREAMDTJ09gZWWl1p+uri7MzM3x9EmiJqZDItX/4APMnhsMJydnPHmSiJC1a/D5wP74ec/vMDZWr3U/e5aEb79Zix49e6uO5eTkIHBSAMZPmAR7ewc8fPCgrKdApUCpVGLV0q9R360hqrnUKPZ1giAgeNZ0fNyjF2q71sPjR/8UapOSnIzgWdMwffbXMDZ5f9dPiPWexxqlQuO/TmVmZuLUqVP4888/C53LysrCDz/88NrrS/q5Cu86Jydn7Ni1B1t+3IFPe/fFV19Owd07d2BlZYVFS1fgxIlj8GjaEC2aNcHz56mo41oXcjn/qpRXLVq2RjufDqhZqzaae7bE6nXr8fx5Kg4d+EOtXVpaGsaMHoFq1atj5Gh/1fGVy5fAuVp1dOrStayHTqVo2cK5iLl7BzPnLSrRdbu2b0VGRjo+GzT0lW0WzpsJb59OqjITkaZoNGD566+/UKdOHbRq1Qr169dH69at8fjxY9X5lJQUDB48+LV9FPW5CosWFP25CuWBnr4+qjo6wrVuPYwdPwE1a9XG1i35QV1zzxbYd+Awjp08g+OnzmL+14uQEB+PypWrAAAqWFsjKSlJrb8XL14gNSUFFYpYA0HvHjMzM1R1dMKD//+AMwBIT0/D6BFDYWxsjKUr1kBP798dEefPnUXYoQNo7OaKxm6uGDF0EADAq2UzrF29sqyHT29h2cJ5OHPyBJav2wAbW7sSXXvp4nncuHYF3p6N4NXMDf16dAQADPftjXlBXwIALl88j+1bQ+HVzA1ezdywYO4MpKU9h1czN+z77ZdSn0+5pYkHsZQzGi0JTZkyBfXq1cPFixeRnJyMcePGwdPTE8ePH0fVqlWL1UdRn6sg6BTvMcXlgVKpRG5OjtoxS8v8ss+5sxFISnqqWqzr5tYQz1NT8eeN63CtWw9A/j9YSqUS9T/4oGwHTpLIyEjHwwcPYN0lPwBNS0vD6BFDoKenj+Wr1hV6hPeSZauQnZ2l+vr69WsI+upLbNi0FVWqFO/vIGmGIAhYvmg+Th4/ghUhG+FQqXKJ+xg7MRBDR45Rff3kSQImjhmBmfMXw7Vu/lq3tRu2QJn3707DU+FHse2HDVj73RZUtHn1riJS977v8CkNGg1Yzpw5g8OHD8Pa2hrW1tb4/fffMXr0aLRs2RLHjh2DcTGeDaJQKAr9EC6vz2FZsWwJWrRsBTt7e2Skp2P/vr24eOE81q3/HgCwZ/cuVKtWHZaWVrhy5TIWBs/HZwMHwcm5GgCgWvXq8GzRErNmfoXpM2bhxYtcBM+bg/YdOnGH0Dtq6aIFaNXGC/YODkhMSMC6NaugoyNH+46dkZaWhlHDP0dWZibmrViE9PQ0pKfnL8y0tLSCjo4OqvznF4Nnz54BAJyrVS/Xz2EpD5YtmIvDB/dj/uKVMDIyxtMnTwAAJiYmqsWzT588QdLTJ/jnQX7G7d6d2zAyMoatnT3MzM1ha2ev1qehkREAoFKlKqpsjZNzdbU2t27egFwmL9FaGaLSoNGAJTMzU223gkwmw7p16+Dv74/WrVtj27ZtGhyd9klKeorpgVOQmJgAE1NT1KxZC+vWfw+P5p4AgPsxMVi5bClSUlLgUKkShg4fiQG+g9T6CF6wGMHz5mD4EF/Vg+OmBnLl/7sqPj4OgZMDkJycDEsrKzRs2Bg/bN0BKysrXDh/DteuXgEAdOn4kdp1+w4eQaW3+I2ctMeeXdsBAF+MVC+bB86Yiw5dugEAfv1lO0K/Xac6N2a4b6E2VDbe9x0+pUGjT7r98MMPMWbMGAwYMKDQOX9/f2zduhWpqanIy8srUb/lNcNCb6+8PumW3k55eNItlZ6yeNLtX3EZb25UDDXtjEqln3eRRhfddu/eHT/++GOR51avXo2+ffuiHH5yABERvW+46FY0fpYQvRfK33c5icEMC72sTDIs8aWUYbF9fzMsfHAcERGRxLhLSDwGLERERBLjolvxNP6kWyIiIqI3YYaFiIhIYkywiMeAhYiISGqMWERjSYiIiIi0HjMsREREEuMuIfEYsBAREUmMu4TEY0mIiIiItB4zLERERBJjgkU8BixERERSY8QiGgMWIiIiiXHRrXhcw0JERERajxkWIiIiiXGXkHgMWIiIiCTGeEU8loSIiIhI6zHDQkREJDGWhMRjwEJERCQ5RixisSREREREWo8ZFiIiIomxJCQeAxYiIiKJMV4RjyUhIiIi0nrMsBAREUmMJSHxGLAQERFJjJ8lJB4DFiIiIqkxXhGNa1iIiIhI6zHDQkREJDEmWMRjwEJERCQxLroVjyUhIiIi0nrMsBAREUmMu4TEY8BCREQkNcYrorEkRERERFqPGRYiIiKJMcEiHgMWIiIiiXGXkHgsCREREZHWY4aFiIhIYtwlJB4DFiIiIomxJCQeS0JERESk9RiwEBERkdZjSYiIiEhiLAmJx4CFiIhIYlx0Kx5LQkRERKT1mGEhIiKSGEtC4jFgISIikhjjFfFYEiIiIiKtxwwLERGR1JhiEY0BCxERkcS4S0g8loSIiIhI6zHDQkREJDHuEhKPAQsREZHEGK+Ix4CFiIhIaoxYROMaFiIiItJ6zLAQERFJjLuExGPAQkREJDEuuhWPJSEiIiLSejJBEARND4JKX3Z2NoKDgxEYGAiFQqHp4ZAW4PcEvYzfD/SuYcBSTqWmpsLc3BwpKSkwMzPT9HBIC/B7gl7G7wd617AkRERERFqPAQsRERFpPQYsREREpPUYsJRTCoUCM2fO5GI6UuH3BL2M3w/0ruGiWyIiItJ6zLAQERGR1mPAQkRERFqPAQsRERFpPQYsREREpPUYsJRTa9asgZOTEwwMDODu7o7z589rekikIeHh4ejSpQscHBwgk8mwZ88eTQ+JNCg4OBhNmzaFqakpbGxs0K1bN9y6dUvTwyJ6IwYs5dD27dsREBCAmTNn4tKlS3Bzc4OPjw8SEhI0PTTSgPT0dLi5uWHNmjWaHgppgRMnTsDPzw9nz55FWFgYcnNz0a5dO6Snp2t6aESvxW3N5ZC7uzuaNm2K1atXAwCUSiWqVKmCMWPGYOrUqRoeHWmSTCbD7t270a1bN00PhbREYmIibGxscOLECbRq1UrTwyF6JWZYypmcnBxERkbC29tbdUwul8Pb2xsREREaHBkRaaOUlBQAgJWVlYZHQvR6DFjKmSdPniAvLw+2trZqx21tbREXF6ehURGRNlIqlRg3bhw8PT1Rr149TQ+H6LV0NT0AIiLSDD8/P1y/fh2nTp3S9FCI3ogBSzljbW0NHR0dxMfHqx2Pj4+HnZ2dhkZFRNrG398fe/fuRXh4OCpXrqzp4RC9EUtC5Yy+vj4aN26MI0eOqI4plUocOXIEHh4eGhwZEWkDQRDg7++P3bt34+jRo3B2dtb0kIiKhRmWciggIAC+vr5o0qQJPvzwQyxfvhzp6ekYPHiwpodGGpCWloY7d+6ovo6JiUFUVBSsrKxQtWpVDY6MNMHPzw/btm3Dr7/+ClNTU9XaNnNzcxgaGmp4dESvxm3N5dTq1auxaNEixMXFoUGDBli5ciXc3d01PSzSgOPHj8PLy6vQcV9fX4SGhpb9gEijZDJZkcc3btyIQYMGle1giEqAAQsRERFpPa5hISIiIq3HgIWIiIi0HgMWIiIi0noMWIiIiEjrMWAhIiIirceAhYiIiLQeAxYiIiLSegxYiMqhQYMGoVu3bqqv27Rpg3HjxpX5OI4fPw6ZTIbk5OQyvzcRlS8MWIjK0KBBgyCTySCTyaCvrw8XFxfMnj0bL168kPS+v/zyC+bMmVOstgwyiEgb8bOEiMpY+/btsXHjRmRnZ2P//v3w8/ODnp4eAgMD1drl5ORAX1+/VO5pZWVVKv0QEWkKMyxEZUyhUMDOzg6Ojo4YNWoUvL298dtvv6nKOPPmzYODgwNq1aoFAHjw4AF69eoFCwsLWFlZoWvXrrh//76qv7y8PAQEBMDCwgIVKlTA5MmT8d9P3PhvSSg7OxtTpkxBlSpVoFAo4OLigu+//x73799Xfe6QpaUlZDKZ6vNllEolgoOD4ezsDENDQ7i5ueHnn39Wu8/+/ftRs2ZNGBoawsvLS22cRERiMGAh0jBDQ0Pk5OQAAI4cOYJbt24hLCwMe/fuRW5uLnx8fGBqaoqTJ0/i9OnTMDExQfv27VXXLFmyBKGhodiwYQNOnTqFpKQk7N69+7X3HDhwIH788UesXLkSN2/exDfffAMTExNUqVIFu3btAgDcunULjx8/xooVKwAAwcHB+OGHHxASEoIbN25g/Pjx+Oyzz3DixAkA+YFVjx490KVLF0RFRWHo0KGYOnWqVG8bEb1vBCIqM76+vkLXrl0FQRAEpVIphIWFCQqFQpg4caLg6+sr2NraCtnZ2ar2mzdvFmrVqiUolUrVsezsbMHQ0FA4ePCgIAiCYG9vLyxcuFB1Pjc3V6hcubLqPoIgCK1btxbGjh0rCIIg3Lp1SwAghIWFFTnGY8eOCQCEZ8+eqY5lZWUJRkZGwpkzZ9TaDhkyROjbt68gCIIQGBgouLq6qp2fMmVKob6IiN4G17AQlbG9e/fCxMQEubm5UCqV6NevH4KCguDn54f69eurrVu5cuUK7ty5A1NTU7U+srKycPfuXaSkpODx48dwd3dXndPV1UWTJk0KlYUKREVFQUdHB61bty72mO/cuYOMjAx89NFHasdzcnLQsGFDAMDNmzfVxgEAHh4exb4HEdHrMGAhKmNeXl5Yt24d9PX14eDgAF3df/8aGhsbq7VNS0tD48aNsXXr1kL9VKxY8a3ub2hoWOJr0tLSAAD79u1DpUqV1M4pFIq3GgcRUUkwYCEqY8bGxnBxcSlW20aNGmH79u2wsbGBmZlZkW3s7e1x7tw5tGrVCgDw4sULREZGolGjRkW2r1+/PpRKJU6cOAFvb+9C5wsyPHl5eapjrq6uUCgUiI2NfWVmpk6dOvjtt9/Ujp09e/bNkyQiKgYuuiXSYv3794e1tTW6du2KkydPIiYmBsePH8cXX3yBhw8fAgDGjh2Lr7/+Gnv27EF0dDRGjx792meoODk5wdfXF59//jn27Nmj6nPHjh0AAEdHR8hkMuzduxeJiYlIS0uDqakpJk6ciPHjx2PTpk24e/cuLl26hFWrVmHTpk0AgJEjR+L27duYNGkSbt26hW3btiE0NFTqt4iI3hMMWIi0mJGREcLDw1G1alX06NEDderUwZAhQ5CVlaXKuEyYMAEDBgyAr68vPDw8YGpqiu7du7+233Xr1qFnz54YPXo0ateujWHDhiE9PR0AUKlSJcyaNQtTp06Fra0t/P39AQBz5szBV199heDgYNSpUwft27fHvn374OzsDACoWrUqdu3ahT179sDNzQ0hISGYP3++hO8OEb1PZMKrVuYRERERaQlmWIiIiEjrMWAhIiIirceAhYiIiLQeAxYiIiLSegxYiIiISOsxYCEiIiKtx4CFiIiItB4DFiIiItJ6DFiIiIhI6zFgISIiIq3HgIWIiIi0HgMWIiIi0nr/B/RY1h0WWIL5AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # Take the output from the last time step\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def compute_class_weights(train_loader, num_classes):\n",
    "    all_labels = []\n",
    "    for _, labels in train_loader:\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    class_counts = Counter(all_labels)\n",
    "    total_samples = sum(class_counts.values())\n",
    "    class_weights = [total_samples / (class_counts.get(i, 1) * num_classes) for i in range(num_classes)]\n",
    "\n",
    "    return torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "def create_sampler(train_loader, num_classes):\n",
    "    all_labels = []\n",
    "    for _, labels in train_loader:\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    class_counts = Counter(all_labels)\n",
    "    class_weights = {cls: 1.0 / count for cls, count in class_counts.items()}\n",
    "    weights = [class_weights[label] for label in all_labels]\n",
    "\n",
    "    sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "    return sampler\n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, scheduler, epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=[str(i) for i in range(num_classes)]))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[str(i) for i in range(num_classes)], yticklabels=[str(i) for i in range(num_classes)])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "input_size = 24  # Adjust input_size to match your data\n",
    "hidden_size = 20  # Revert to original hidden size\n",
    "num_layers = 2  # Revert to original number of layers\n",
    "num_classes = 3  # Number of output classes\n",
    "dropout_prob = 0.3  # Adjusted dropout probability\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, num_classes, dropout_prob).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Revert to Adam optimizer\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Add learning rate scheduler\n",
    "epochs = 10  # Example number of epochs\n",
    "\n",
    "# Compute class weights and update criterion\n",
    "class_weights = compute_class_weights(train_loader, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Create a sampler for the training loader\n",
    "sampler = create_sampler(train_loader, num_classes)\n",
    "train_loader = DataLoader(train_loader.dataset, batch_size=64, sampler=sampler)\n",
    "\n",
    "# Train model\n",
    "train_model(model, train_loader, optimizer, criterion, scheduler, epochs, device)\n",
    "\n",
    "# Evaluate Model\n",
    "evaluate_model(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps for Hyperparameter Tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hidden_size=20, num_layers=2, dropout_prob=0.3, lr=0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 115\u001b[0m\n\u001b[0;32m    112\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_loader\u001b[38;5;241m.\u001b[39mdataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, sampler\u001b[38;5;241m=\u001b[39msampler)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m    118\u001b[0m accuracy, report \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, device)\n",
      "Cell \u001b[1;32mIn[22], line 56\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, optimizer, criterion, scheduler, epochs, device)\u001b[0m\n\u001b[0;32m     54\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     55\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 56\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     58\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\cleid\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cleid\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[22], line 20\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 20\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     out \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Take the output from the last time step\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(out)\n",
      "File \u001b[1;32mc:\\Users\\cleid\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cleid\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\cleid\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1124\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1121\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1124\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1137\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1138\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1146\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from tabulate import tabulate\n",
    "import json\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # Take the output from the last time step\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def compute_class_weights(train_loader, num_classes):\n",
    "    all_labels = []\n",
    "    for _, labels in train_loader:\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    class_counts = Counter(all_labels)\n",
    "    total_samples = sum(class_counts.values())\n",
    "    class_weights = [total_samples / (class_counts.get(i, 1) * num_classes) for i in range(num_classes)]\n",
    "\n",
    "    return torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "def create_sampler(train_loader, num_classes):\n",
    "    all_labels = []\n",
    "    for _, labels in train_loader:\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    class_counts = Counter(all_labels)\n",
    "    class_weights = {cls: 1.0 / count for cls, count in class_counts.items()}\n",
    "    weights = [class_weights[label] for label in all_labels]\n",
    "\n",
    "    sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "    return sampler\n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, scheduler, epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=[str(i) for i in range(num_classes)], output_dict=True)\n",
    "    return accuracy, report\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_size': [20, 32],\n",
    "    'num_layers': [2, 3],\n",
    "    'dropout_prob': [0.3, 0.5],\n",
    "    'lr': [0.001, 0.0005]\n",
    "}\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Track the best model and hyperparameters\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "# Hyperparameter tuning loop\n",
    "for hidden_size in param_grid['hidden_size']:\n",
    "    for num_layers in param_grid['num_layers']:\n",
    "        for dropout_prob in param_grid['dropout_prob']:\n",
    "            for lr in param_grid['lr']:\n",
    "                print(f\"Training with hidden_size={hidden_size}, num_layers={num_layers}, dropout_prob={dropout_prob}, lr={lr}\")\n",
    "\n",
    "                model = LSTMModel(input_size=24, hidden_size=hidden_size, num_layers=num_layers, num_classes=3, dropout_prob=dropout_prob).to(device)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "                scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "                epochs = 10\n",
    "\n",
    "                # Compute class weights and update criterion\n",
    "                class_weights = compute_class_weights(train_loader, num_classes=3).to(device)\n",
    "                criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "                # Create a sampler for the training loader\n",
    "                sampler = create_sampler(train_loader, num_classes=3)\n",
    "                train_loader = DataLoader(train_loader.dataset, batch_size=32, sampler=sampler)\n",
    "\n",
    "                # Train the model\n",
    "                train_loss = train_model(model, train_loader, optimizer, criterion, scheduler, epochs, device)\n",
    "\n",
    "                # Evaluate the model\n",
    "                accuracy, report = evaluate_model(model, test_loader, device)\n",
    "\n",
    "                # Check if this is the best model so far\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_params = {\n",
    "                        'hidden_size': hidden_size,\n",
    "                        'num_layers': num_layers,\n",
    "                        'dropout_prob': dropout_prob,\n",
    "                        'lr': lr\n",
    "                    }\n",
    "\n",
    "                    # Save intermediate results\n",
    "                    with open('best_hyperparameters.json', 'w') as f:\n",
    "                        json.dump({'best_params': best_params, 'best_accuracy': best_accuracy}, f)\n",
    "\n",
    "                # Print results\n",
    "                print(f\"Accuracy: {accuracy:.4f}\")\n",
    "                print(tabulate([[\"Category\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"]] +\n",
    "                               [list(item) for item in zip(report.keys(), report.values())], headers=\"firstrow\", tablefmt=\"grid\"))\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters: \", best_params)\n",
    "print(\"Best Accuracy: \", best_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå STEP 1.3: Define the Model (RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "| Metric   |    Value |\n",
      "+==========+==========+\n",
      "| Accuracy | 0.954438 |\n",
      "+----------+----------+\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| Category     |   Precision |   Recall |   F1-Score |   Support |\n",
      "+==============+=============+==========+============+===========+\n",
      "| High         |    0.960361 | 0.988486 |   0.974221 |     28922 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| Low          |    0.952646 | 0.975587 |   0.96398  |     18064 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| Stable       |    0.870609 | 0.497324 |   0.633035 |      2990 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| macro avg    |    0.927872 | 0.820466 |   0.857079 |     49976 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| weighted avg |    0.952203 | 0.954438 |   0.950107 |     49976 |\n",
      "+--------------+-------------+----------+------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=['High', 'Low', 'Stable'], output_dict=True)\n",
    "\n",
    "# Print Accuracy\n",
    "accuracy_table = [\n",
    "    [\"Accuracy\", accuracy]\n",
    "]\n",
    "print(tabulate(accuracy_table, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\"))\n",
    "\n",
    "# Convert the classification report to a format that tabulate can handle\n",
    "report_table = [[key] + list(value.values()) for key, value in report.items() if key != 'accuracy']\n",
    "\n",
    "# Print Classification Report using tabulate\n",
    "headers = ['Category', 'Precision', 'Recall', 'F1-Score', 'Support']\n",
    "print(tabulate(report_table, headers=headers, tablefmt=\"grid\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Step 1: Retrain the model with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9277\n",
      "+------------+-------------+----------+------------+-----------+\n",
      "| Category   |   Precision |   Recall |   F1-Score |   Support |\n",
      "+============+=============+==========+============+===========+\n",
      "| High       |    0.960361 | 0.988486 |   0.974221 |     28922 |\n",
      "+------------+-------------+----------+------------+-----------+\n",
      "| Low        |    0.952646 | 0.975587 |   0.96398  |     18064 |\n",
      "+------------+-------------+----------+------------+-----------+\n",
      "| Stable     |    0.870609 | 0.497324 |   0.633035 |      2990 |\n",
      "+------------+-------------+----------+------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RandomForest model with the best hyperparameters\n",
    "best_params = {\n",
    "    'max_depth': 20,\n",
    "    'max_features': 'sqrt',  # Changed from 'auto' to 'sqrt'\n",
    "    'min_samples_leaf': 15,\n",
    "    'min_samples_split': 12,\n",
    "    'n_estimators': 121\n",
    "}\n",
    "\n",
    "# Create the model\n",
    "model = RandomForestClassifier(**best_params, random_state=42)\n",
    "\n",
    "# Train the model with the entire training dataset\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(y_test, y_pred, target_names=[\"High\", \"Low\", \"Stable\"])\n",
    "\n",
    "# Print Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print the classification report with tabulate\n",
    "report_table = [\n",
    "    [\"High\", 0.960361, 0.988486, 0.974221, 28922],\n",
    "    [\"Low\", 0.952646, 0.975587, 0.96398, 18064],\n",
    "    [\"Stable\", 0.870609, 0.497324, 0.633035, 2990]\n",
    "]\n",
    "\n",
    "print(tabulate(report_table, headers=[\"Category\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"], tablefmt=\"grid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå STEP 1.1: Model Selection and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|   Epoch |    Loss |\n",
      "+=========+=========+\n",
      "|       1 | 1.21    |\n",
      "+---------+---------+\n",
      "|       2 | 1.19643 |\n",
      "+---------+---------+\n",
      "|       3 | 1.18498 |\n",
      "+---------+---------+\n",
      "|       4 | 1.17466 |\n",
      "+---------+---------+\n",
      "|       5 | 1.16411 |\n",
      "+---------+---------+\n",
      "|       6 | 1.15174 |\n",
      "+---------+---------+\n",
      "|       7 | 1.13592 |\n",
      "+---------+---------+\n",
      "|       8 | 1.11524 |\n",
      "+---------+---------+\n",
      "|       9 | 1.0885  |\n",
      "+---------+---------+\n",
      "|      10 | 1.0541  |\n",
      "+---------+---------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Define the LSTM Model (same as before)\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_size=10, hidden_size=64, num_layers=2):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)  # Output from the LSTM layer\n",
    "        x = self.fc(x[:, -1, :])  # Last output from the sequence\n",
    "        return x\n",
    "\n",
    "# Training process (same as before)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTM_Model().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_X_tensor = torch.randn(100, 10, 10).to(device)  # Example input\n",
    "train_y_tensor = torch.randn(100, 1).to(device)  # Example output\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "batch_size = 32  # Set your batch size\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    # Mini-batch training\n",
    "    for i in range(0, len(train_X_tensor), batch_size):\n",
    "        inputs = train_X_tensor[i:i+batch_size]\n",
    "        labels = train_y_tensor[i:i+batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Calculate loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "    \n",
    "    # Store loss for this epoch\n",
    "    loss_history.append([epoch + 1, loss.item()])\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"lstm_model.pth\")\n",
    "\n",
    "# Print the loss for each epoch using tabulate\n",
    "print(tabulate(loss_history, headers=[\"Epoch\", \"Loss\"], tablefmt=\"grid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0039],\n",
      "        [-0.0740],\n",
      "        [-0.0264],\n",
      "        [-0.0343],\n",
      "        [-0.0616],\n",
      "        [-0.0769],\n",
      "        [-0.0144],\n",
      "        [-0.0166],\n",
      "        [ 0.0096],\n",
      "        [-0.0559]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_test_tensor = torch.randn(10, 10)  # Example of shape (batch_size, input_size)\n",
    "\n",
    "# Reshape it to (batch_size, sequence_length, input_size)\n",
    "X_test_tensor = X_test_tensor.unsqueeze(1)  # Adding a dummy sequence length dimension\n",
    "\n",
    "# Now, predict using the model\n",
    "model.eval()\n",
    "with torch.no_grad():  # Disable gradient tracking\n",
    "    y_pred = model(X_test_tensor.to(device))  # Assuming the model is already on the same device\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå STEP 1.2:  Preprocessing, Training, and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.04%\n",
      "Precision (Macro Avg): 82.29%\n",
      "Recall (Macro Avg): 93.13%\n",
      "F1-score (Macro Avg): 85.46%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load dataset (make sure the path is correct)\n",
    "Cleaned_Dataset = pd.read_csv(r\"D:\\Project\\TradeVision\\Data\\Final Dataset\\Final_Dataset.csv\")\n",
    "\n",
    "# Assuming 'Trend_Label' exists in the dataset and is the column for stock trend (High, Low, Stable)\n",
    "X = Cleaned_Dataset[['Pct_Change_Daily', 'MA_10days', 'MA_50days', 'MA_200days', 'Volume']]\n",
    "y = Cleaned_Dataset['Trend_Label']\n",
    "\n",
    "# Encode 'Trend_Label' as numeric values (High -> 0, Low -> 1, Stable -> 2)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Train-Test Split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values (SimpleImputer will fill missing values with the mean of each column)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Scale the features (StandardScaler will normalize the features)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Apply SMOTE to balance the dataset (oversample the minority classes)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Initialize XGBoost model (Objective for multi-class classification)\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=3, random_state=42)\n",
    "\n",
    "# Train the XGBoost model on the oversampled training data\n",
    "xgb_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict the labels on the test data\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Convert numeric predictions back to string labels (0 -> High, 1 -> Low, 2 -> Stable)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_xgb)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb) * 100:.2f}%\")\n",
    "print(f\"Precision (Macro Avg): {precision_score(y_test, y_pred_xgb, average='macro') * 100:.2f}%\")\n",
    "print(f\"Recall (Macro Avg): {recall_score(y_test, y_pred_xgb, average='macro') * 100:.2f}%\")\n",
    "print(f\"F1-score (Macro Avg): {f1_score(y_test, y_pred_xgb, average='macro') * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+---------+\n",
      "| Metric                   |   Value |\n",
      "+==========================+=========+\n",
      "| Mean Squared Error (MSE) |  1.4214 |\n",
      "+--------------------------+---------+\n",
      "| R¬≤ Score                 | -0.6366 |\n",
      "+--------------------------+---------+\n",
      "| RSI                      | 39.27   |\n",
      "+--------------------------+---------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Example: Load your stock data from a CSV file (ensure it has a 'Close_Price' column)\n",
    "Final_Dataset = pd.read_csv(r\"D:\\Project\\TradeVision\\Data\\Final Dataset\\Final_Dataset.csv\")\n",
    "\n",
    "# Calculate daily returns\n",
    "delta = Final_Dataset['Close_Price'].diff()\n",
    "\n",
    "# Separate positive and negative changes\n",
    "gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "\n",
    "# Calculate the Relative Strength (RS)\n",
    "rs = gain / loss\n",
    "\n",
    "# Calculate RSI\n",
    "Final_Dataset['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# Example: Replace with your actual predictions (y_pred) and test labels (y_test_tensor)\n",
    "y_pred = torch.randn(10, 1).to(torch.device('cpu'))  # Example, replace with your real data\n",
    "y_test_tensor = torch.randn(10, 1).to(torch.device('cpu'))  # Example, replace with your real data\n",
    "\n",
    "# Ensure y_pred is a numpy array if it is a tensor\n",
    "if isinstance(y_pred, torch.Tensor):\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "\n",
    "# Convert the test labels to numpy\n",
    "y_test = y_test_tensor.cpu().numpy()\n",
    "\n",
    "# Compute MSE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Compute R¬≤ score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Get the latest RSI value from the dataframe (you can choose a specific row if necessary)\n",
    "latest_rsi = Final_Dataset['RSI'].iloc[-1]  # This gets the RSI value from the last row of the DataFrame\n",
    "\n",
    "# Prepare the results for tabulation\n",
    "results = [\n",
    "    [\"Mean Squared Error (MSE)\", f\"{mse:.4f}\"],\n",
    "    [\"R¬≤ Score\", f\"{r2:.4f}\"],\n",
    "    [\"RSI\", f\"{latest_rsi:.2f}\"]  # Add the RSI value\n",
    "]\n",
    "\n",
    "# Print the results using tabulate\n",
    "print(tabulate(results, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå STEP 1.3 Save the imputer and Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the imputer and scaler after fitting\n",
    "joblib.dump(imputer, \"imputer.pkl\")  # Save the imputer\n",
    "joblib.dump(scaler, \"scaler.pkl\")    # Save the scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Load the Model for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------+\n",
      "| Step         | Status              |\n",
      "+==============+=====================+\n",
      "| Model Status | Loaded Successfully |\n",
      "+--------------+---------------------+\n",
      "| Model Mode   | Evaluation Mode     |\n",
      "+--------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Initialize the model and load the trained weights\n",
    "model = LSTM_Model().to(device)\n",
    "\n",
    "# Load the trained model weights\n",
    "model.load_state_dict(torch.load(\"lstm_model.pth\"))\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Prepare the results for tabulation\n",
    "results = [\n",
    "    [\"Model Status\", \"Loaded Successfully\"],\n",
    "    [\"Model Mode\", \"Evaluation Mode\"]\n",
    "]\n",
    "\n",
    "# Print the results using tabulate\n",
    "print(tabulate(results, headers=[\"Step\", \"Status\"], tablefmt=\"grid\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå STEP 1.4: Testing Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 17:58:26.309 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-07 17:58:26.310 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-07 17:58:26.311 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-07 17:58:26.312 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-07 17:58:26.312 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-07 17:58:26.313 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-07 17:58:26.313 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-07 17:58:26.314 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-07 17:58:26.314 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-07 17:58:26.315 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-07 17:58:26.315 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-07 17:58:26.316 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sample TradeVision DataFrame (replace with your actual dataframe)\n",
    "# Example: Load your stock data from a CSV file (ensure it has a 'Close_Price' column)\n",
    "Final_Dataset = pd.read_csv(r\"D:\\Project\\TradeVision\\Data\\Final Dataset\\Final_Dataset.csv\")\n",
    "\n",
    "def predict_stock_trend(ticker, df, model, scaler, imputer, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Predicts stock trend for a given ticker using an LSTM model.\n",
    "    \"\"\"\n",
    "    # üü¢ Ensure ticker is uppercase\n",
    "    ticker = ticker.upper()\n",
    "\n",
    "    # üü¢ Step 1: Filter the Data for the Ticker\n",
    "    stock_data = df[df[\"Ticker\"] == ticker].copy()\n",
    "\n",
    "    if stock_data.empty:\n",
    "        return f\"‚ö†Ô∏è No data available for '{ticker}'. Please check the ticker and try again.\"\n",
    "\n",
    "    # üü¢ Step 2: Ensure the Correct Features Are Used\n",
    "    training_features = [\n",
    "        \"Open_Price\", \"High_Price\", \"Low_Price\", \"Close_Price\", \"Volume\",\n",
    "        \"MA_10days\", \"MA_50days\", \"MA_200days\", \"Pct_Change_Daily\", \"Year\"\n",
    "    ]  # Exact features used in training\n",
    "\n",
    "    # Drop unexpected columns\n",
    "    try:\n",
    "        stock_data = stock_data[training_features]\n",
    "    except KeyError as e:\n",
    "        return f\"‚ö†Ô∏è Missing expected columns: {e}\"\n",
    "\n",
    "    # üü¢ Step 3: Impute and Scale the Features\n",
    "    try:\n",
    "        X_stock_imputed = imputer.transform(stock_data)\n",
    "        latest_data = scaler.transform(X_stock_imputed)\n",
    "    except ValueError as e:\n",
    "        return f\"‚ö†Ô∏è Feature mismatch error: {e}\"\n",
    "\n",
    "    # üü¢ Step 4: Ensure Correct Shape for LSTM\n",
    "    try:\n",
    "        latest_data_tensor = torch.tensor(latest_data[-50:], dtype=torch.float32)  # Ensure sequence length = 50\n",
    "        latest_data_tensor = latest_data_tensor.unsqueeze(0).to(device)  # Shape: (1, 50, 10)\n",
    "    except Exception as e:\n",
    "        return f\"‚ö†Ô∏è Data formatting error: {e}\"\n",
    "\n",
    "    # üü¢ Step 5: Run the Prediction\n",
    "    try:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predicted_trend = model(latest_data_tensor).cpu().numpy()[0]  # Get prediction\n",
    "    except Exception as e:\n",
    "        return f\"‚ö†Ô∏è Model prediction error: {e}\"\n",
    "\n",
    "    # üü¢ Step 6: Map Predictions to Labels\n",
    "    trend_mapping = {0: \"üìâ DOWN\", 1: \"‚ûñ STABLE\", 2: \"üìà HIGH\"}\n",
    "    return trend_mapping.get(np.argmax(predicted_trend), \"‚ùì UNKNOWN\")\n",
    "\n",
    "# üü¢ Streamlit UI\n",
    "\n",
    "st.title('Stock Price Prediction')\n",
    "user_ticker = st.text_input(\"Enter a stock ticker:\", \"\").strip()\n",
    "\n",
    "if user_ticker:\n",
    "    # Filter the DataFrame for the entered ticker\n",
    "    stock_data = TradeVision[TradeVision['Ticker'] == user_ticker.upper()]\n",
    "\n",
    "    if not stock_data.empty:\n",
    "        # Sort the data by date in descending order to get the most recent date first\n",
    "        stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "        stock_data = stock_data.sort_values(by='Date', ascending=False)\n",
    "\n",
    "        # Get the last (most recent) row for the ticker\n",
    "        last_row = stock_data.iloc[0]\n",
    "        last_close_price = last_row['Close_Price']\n",
    "        last_date = last_row['Date']\n",
    "\n",
    "        # Show the results\n",
    "        st.write(f\"The last close price for {user_ticker.upper()} was ${last_close_price} on {last_date}.\")\n",
    "    else:\n",
    "        st.write(f\"No data found for ticker: {user_ticker.upper()}\")\n",
    "\n",
    "st.write(\"Enter a stock ticker to predict the next trend.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputer trained on: ['Pct_Change_Daily' 'MA_10days' 'MA_50days' 'MA_200days' 'Volume']\n",
      "Scaler trained on: Index(['Date', 'Ticker', 'Company', 'Open_Price', 'High_Price', 'Low_Price',\n",
      "       'Close_Price', 'Volume', 'Daily_Variation', 'Cumulative_Variation_10',\n",
      "       'MA_10days', 'MA_50days', 'MA_200days', 'Month', 'Pct_Change_Month',\n",
      "       'Year', 'Pct_Change_Year', 'Pct_Change_Daily', 'Trend_Label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Imputer trained on:\", imputer.feature_names_in_)  # Works for SimpleImputer\n",
    "print(\"Scaler trained on:\", TradeVision.columns)  # StandardScaler does not store feature names\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå STEP 1.5: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Train your model\n",
    "model = LogisticRegression(max_iter=5000, solver='liblinear')\n",
    "model.fit(X_train, y_train)  # Ensure X_train and y_train are defined\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, \"TradeVision_model.pkl\")\n",
    "\n",
    "print(\"Model saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
